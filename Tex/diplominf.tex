\documentclass[hyperref,german,diplominf]{cgvpub}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig

\usepackage{lmodern}
%\usepackage[ngerman]{babel}

\author{Josef Schulz}
\title{Optimierung und Übertragung von Tiefengeometrie für Remote-Visualisierung}
\birthday{20. Oktober 1989}
\placeofbirth{Naumburg (Saale)}
\matno{3658867}
\betreuer{Dr. Sebastian Grottel}
\bibfiles{literatur}

\problem{
In Big-Data-Szenarien in der Visualisierung spielt der Ansatz der Remote-Visualisierung eine zunehmende Rolle.  
Moderne Netzwerktechnologien bieten große Datenübertragungsraten und niedrige Latenzzeiten. Für die 
interaktive Visualisierung sind aber selbst kleinste Latenzzeiten problematisch. Um diese vor dem Benutzer maskieren zu können, kann eine Extrapolation der Darstellung durchgeführt. 
Diese Berechnungen erfordern zusätzlich zum normalen Farbbild weitere Daten, beispielsweise 
ein Tiefenbild und die Daten der verwendeten Kameraeinstellung.
Für die Darstellungsextrapolation werden Farb- und Tiefenbild zusammen interpretiert, beispielsweise als Punktwolke oder Höhenfeldgeometrie. 
Im Rahmen dieser Arbeit soll untersucht werden, wie die Darstellung mittels Höhenfeldgeometrie optimiert  werden kann. 
Ansätze sind hierfür Algorithmen aus der Netzvereinfachung. Zu erwarten sind sowohl 
harte Kanten als auch glatte Verläufe der Tiefenwerte, welche sich in der Netzgeometrie durch 
adaptive Vernetzung mit reduziertem Datenaufwand darstellen lassen.


Dem Szenario der Web-basierten Remote-Visualisierung folgend soll der Web-Browser als
Klient-Komponente eingesetzt werden. Die einzusetzenden Technologien sind HTML5, Javascript, 
WebGL und WebSockets. Entsprechende Javascript-Bibliotheken sollen genutzt 
werden um die Qualität und Wartbarkeit des Quellcodes zu steigern. Für die Server-Komponente darf die Technologie vom Bearbeiter frei gewählt werden.

Zu Beginn der Arbeit wird eine Literatur-Recherche zu Web-basierter Visualisierung und Remote-Visualisierung erfolgen. 
Schwerpunkte  sind hierbei  die  Bild-Extrapolation, Vernetzung 
und Rekonstruktion auf Basis von Tiefenbildern und die Netzoptimierung und -Vereinfachung. 
Im Anschluss an die Literaturrecherche wird ein Konzept für die Implementierung mit dem 
Betreuer abgesprochen und anschließend als prototypische Software umgesetzt. Folgendes 
Szenario dient als Grundlage für dieses Konzept:

Als  Eingabedaten  stehen  mehrere  Datensätze aus  unterschiedlichen  Szenarien  der  wissenschaftlichen Visualisierung zur Verfügung. Für jeden Datensatz sind mehrere Tripel aus Farbbild, Tiefenbild und Kamera-Parameter gegeben.
Die Serverkomponente bereitet einen Datensatz auf und bietet ihn dem Klienten an. Diese Aufbereitung ist vor allem die Generierung einer optimierten  Tiefennetzgeometrie  aus  den  Tiefenbilddaten.  Der  Klient  fordert  Farbbilder,  Kameraeinstellungen und Tiefengeometrie von Tripel-Paaren an.
Konzeptuell wird ein Tripel als aktueller Zustand und das zweite Tripel als Ground-Truth einer Bildextrapolation verstanden. 
Diese können daher auch in dieser Reihenfolge angefordert werden. 
Die Tripel werden zwischen  Klient  und  Server  direkt  per  Sockets/WebSockets  übertragen.
Die Daten des ersten Tripels werden anschließend genutzt um dessen Farbbild in die Ansicht des zweiten Tripels extrapoliert. Hierbei werden vom zweiten Tripel nur die Kameraeinstellung genutzt.
Diese Extrapolation wird  Klient-seitig in WebGL implementiert  damit  alle  Berechnungen  auf  der  GPU 
ausgeführt werden. 
Anschließend wird das extrapolierte Bild mit dem originalen Ground-Truth-Farbbild  aus  dem  zweiten  Tripel  verglichen  um  die  Qualität  der  Extrapolation  zu  bewerten, z.B. durch SSIM.

Die umgesetzte Lösung wird ausführlich evaluiert.
Zentraler Wert ist hierbei die Bildqualität nach der Extrapolation abhängig vom Winkelunterschied zwischen den Kameraeinstellungen und den Parametern der Vereinfachung der Tiefennetzgeometrie. 
Hierfür werden Tripel-Paare aus den Datensätzen und Variationen der Parameter der Algorithmen systematisch 
und automatisiert vermessen. Untersuchungen zum Laufzeitverhalten der Netzoptimierung im Server 
und der Bildextrapolation im Klienten sind optional durchzuführen.
}

\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}
\acknowledgments{Die Danksagung...}
\abstracten{abstract text english}
\abstractde{ Zusammenfassung Text Deutsch}

\usepackage{pgfplots}
%\usepackage{pgf-umlsd}
\usepackage{filecontents}
\usepackage{subfigure}

\input{../results/4/512x512_Delaunay/D10/L0.0/I0.0/results.tex}



\input{../results/d_0.tex}

\begin{document}

\chapter{Einleitung}

Bei der Remote-Visualisierung, wird die Bildsynthese und die eigentliche Darstellung voneinander getrennt.
Der Server-Prozess erzeugt und kodiert jedes Bild zu einem kompakten Datenpaket, welches im Anschluss
an den Klient-Prozess gesendet, von diesem dekodiert und auf einem Bildschirm ausgegeben wird.
Remote-Visualisierung ist ein insbesondere für mobile Endgeräte interessantes Konzept,
weil es die Visualisierung von komplexen Szenen auch auf Leistungsarmen Geräten ermöglicht.
Neben Computerspielen, ist die wissenschaftliche Visualisierung ein wichtiges Anwendungsgebiet,
da Datensätze Größenordnungen erreichen, die den Speicher herkömmlicher Desktops, Laptops,
Smartphones etc. bei weitem übersteigen. 
Auch wenn ausreichend Speicher zur Verfügung steht, kann die Übertragung langwierig und teuer werden.
Mit Hilfe der Remote-Visualisierung kann die Visualisierung vom Server übernommen werden und
es muss nicht der komplette Datensatz übertragen werden.
Mit leistungsstarken Serversystemen lassen sich komplexe Beleuchtungsmethoden verwendet,
die mit normalen Endgeräten nicht zu realisieren sind.

Die Latenz bezeichnet in der Netzwerktechnik die Übertragungszeit von einem zum anderen Gerät.
Diese ist in modernen Netzwerken gering, für die Interaktive Visualisierung allerdings immer
noch zu Groß um eine für den Menschen nicht wahrnehmbare flüssige Betrachtung von Szenen
Bild für Bild zu ermöglichen.

Ein Ausweg besteht darin, dass der Klient-Prozess ein bereits empfangenes Bild extrapoliert, 
dadurch kann dem Betrachter ein neues Bild präsentiert werden, obwohl es noch nicht empfangen wurde.
Bei der Bildsynthese des Server-Prozesses entsteht neben dem Farbbild zusätzlich ein Tiefenbild,
das genutzt werden kann um geometrische Informationen an den Klient-Prozess weiter zu reichen
und mit Hilfe dieser Extrapolation durchzuführen.
Aus dem Tiefenbild wir dazu ein Dreiecksnetz erzeugt, das mit dem Farbbild als Textur
vom Klient aus der neuen Kameraperspektive synthetisiert wird.

Zu diesem Zweck wurde eine Server- und eine Klient-Komponente entwickelt, die mit
Hilfe des WebSocket-Protokolls Daten untereinander austauschen.
Der Klient basiert auf JavaScript und nutzt für die Bildextrapolation WebGl.
Die Qualität der Darstellungen werden mit Hilfe von Ground-Truth-Daten überprüft.
Zum Vergleich wird der PSNR und der SSIM \cite{Wang04imagequality} der Bildpaare bestimmt und evaluiert.

Im Folgenden werden existierende Konzepte und Ideen vorgestellt.


\chapter{Verwandte Arbeiten}

Einen Überblick über Architekturen und Methoden der interaktiven Remote-Visualisierung geben Shu Shi et al. \cite{Shi:2015:SIR:2775083.2719921}.
Zum zentralen Problem ihrer Arbeit, wird die Latenz und die effiziente Übertragung der Daten vom Server zum Klient.
Lösungen hängen vom Anwendungsfall ab, in \textit{THIN}-Systemen besteht die Aufgabe in der Übertragung von
2D Informationen, mit denen sich zum Beispiel Desktop-Anwendungen Fernsteuern lassen.
Als Beispiele dienen SLIM \cite{Schmidt:1999:IPS:319344.319154} und THiNC \cite{Baratto:2005:TVD:1095809.1095837}.
Die Übertragung der Daten wurde in beiden Systemen für den Einsatz von 2D Grafiken optimiert.
Ein Vorteil bei diesen Anwendungen besteht darin, dass nur relativ kleine Änderungen
tatsächlich übertragen werden müssen, wenn sich zum Beispiel ein Fenster ändert muss auch
nur diese Änderung zum Klient übertragen werden.
Zur Verbesserung der Latenz wird von Shu Shi et al. die Bildextrapolation vorgeschlagen.
Diese kann durch die zusätzliche Übertragung von Tiefenbildern oder Dreiecksnetzen erreicht werden.
Bei der Bildsynthese auf dem Klient-System, entsteht eine Menge von Artefakten, diese kann durch Verzerrung und Verformung der Geometrieinformationen verkleinert werden, durch sogenanntes \textit{Warping} \cite{1331221}, \cite{4810995}.
Wenn eine Extrapolation der Bilder, durch den Klient, aus Hardware-Gründen nicht möglich ist,
können auch die nächst möglichen Bewegungen geschätzt werden und deren Ergebnisse werden mit übertragen.
Zur Verbesserung der Qualität werden deshalb in vielen Anwendungen die Bewegungsmöglichkeiten
des Nutzers auf Pfade oder Aussichtspunkte beschränkt.

\section{Architektur}

Ein spezielles Remote-Visualisierungssystem haben Peter Eisert und Philipp Fechteler 
für Computerspiele entwickelt \cite{Eisert07remoterendering}.
Ihr System kommt ohne Beschränkung der Bewegungsfreiheit aus.
Sie haben sich dafür zwei Ansätze zunutze gemacht. 
Im ersten Ansatz erzeugt der Server die fertigen Bilder,
welche mit Video-Codecs codiert und an den Klient gestreamt werden.
Im zweiten Ansatz, werden Zeicheninstruktionen an den Klient gestreamt, welcher mit diesen
das Ergebnissbild produziert.
Ihr System ist für den Einsatz im lokalen Netzwerk konstruiert.

Wessels et al. stellen eine Konzeption für den Programmaufbau eines interaktiven Remote-Visualisierungssystem basierend auf dem WebSocket-Protokoll vor \cite{DBLP:conf/itng/WesselsPJR11}.
In ihrem System besteht der Server-Prozess aus zwei Hauptkomponenten,
der Visualisierungs-Engine und dem Deamon. 
Während die Visualisierungs-Engine für die Bildsynthese zuständig ist, 
übernimmt der Deamon die Kommunikation mit dem Klient-Prozess.
Der Klient schickt dabei seine Eingabeinformationen von Maus und Tastatur direkt an den Server.
Dieser wertet die Daten aus und erzeugt darauf hin ein mit JPEG komprimiertes Bild, das mit Base64 kodiert wird und schließlich an den Klient-Prozess geschickt wird. 
Dieser kann das Bild nativ mit Hilfe eines HTML5 Canvas dekodieren und darstellen.
Ihr System wird zur Grundlage dieser Arbeit.

\section{Extrapolation}

Die Grundidee das Problem der Latenz mit Bildexploration in den Griff zu bekommen wurde in der Arbeit von Palomo et al. betrachtet \cite{Palomo:2010:EAD:1900179.1900236}.

Das erzeugte Tiefenbild lässt sich 
Pauly et al. \cite{Pauly:2002:ESP:602099.602123}

Simon Stegmaier \cite{Stegmaier02ageneric} 
A Generic Solution for Hardware-Accelerated Remote Visualization

\section{Kompression}

Gabriel Taubin und Jarek Rossignac haben ein Algorithmus zur Erzeugung und effizienten Kodierung von Dreiecksstreifen aus Dreiecksnetzen entwickelt \cite{Taubin:1998:GCT:274363.274365}.
Dazu konstruiert ihr Algorithmus Spannbäume über dem Netz, die zur Erzeugung möglichst großer
Dreiecksstreifen genutzt werden. Die Kompression kann wahlweise verlustfrei oder verlustbehaftet
durchgeführt werden. Typische Kompressionsraten werden mit 1:50 angegeben.

Eine weitere Arbeit die sich mit der Kompression von Dreiecksnetzen und einer kompakten Repräsentation
von diesen beschäftigt wurde Stefan Gumhold und Wolfgang Straßer geschrieben \cite{Gumhold:1998:RTC:280814.280836}.
Kompression und Dekompression sind echtzeitfähig.

Michael Deering hat ebenfalls ein Geometrisches Kompressionsverfahren entwickelt. \cite{Deering:1995:GC:218380.218391}

Federico Ponchio  und Matteo Dellepiane \cite{Ponchio:2015:FDW:2775292.2775308}
Fast decompression for web-based view-dependent 3D rendering



Diplomarbeit mädcheninformatiker Effiziente Datenübertragung von Modellen und
Texturen für die Verwendung in WebGL
Stefan Wagner noch kein cite



WEB-BASED VISUALISATION OF ON-SET POINT CLOUD
DATA Alun Evans et al \cite{Evans:2014:WVO:2668904.2668937}

planarer scheiß \cite{Ma13ecmr}

Ein Remote-Visualisierungssystem als ein verteiltes System betrachtet werden.
Zhefan Jin stellt drei Klassen solcher Systeme vor \cite{Jin:2006:RRI:1128923.1128927}.
Diese unterscheiden sich anhand der Verteilung ihrer Daten auf unterschiedliche Host-Systeme.
Dabei können Zeicheninstruktionen, Attribute, oder Bilder von einem Knoten zum anderen weiter gereicht werden.
Auch bei einfache Remote-Visualisierungssysteme unterscheiden sich an Hand der Daten die übertragen werden.

\chapter{Grundlagen}

In diesem Kapitel soll es um die Grundlagen der Arbeit gehen. Es wird
vorgestellt wie ein bestehendes Dreiecksnetz aus einer neuen Perspektive
gezeichnet wird und wie die Methoden zum Bildvergleich im Detail funktionieren.

\section{Extrapolation}

Bei dem Zeichenvorgang auf dem Server entsteht ein Farbbild und ein Tiefenbild.
Das Tiefenbild wurde mit einer Farbtiefe von 16bit erzeugt und ist die Ausgangsbasis
für die Bildextrapolation. 
Da aus diesem ein Dreiecksnetz erzeugt wird, welche das Farbbild als Textur verwendet,
und zur Erzeugung neuer Ansichten genutzt wird.
Um das Verfahren besser zu verständlich zu machen,
wird zunächst die Vertex-Transformation näher betrachtet, zu Verdeutlichung der
Zusammenhänge dient die folgende Abbildung:

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[auto, every node/.style={minimum size=5em, align=center}]
			\node[draw] (A) at (0,0) {Vertex};
			\node[draw] (B) at (4,0) {ModellKamera-\\Matrix};
			
			\node[draw] (C) at (8,0) {Projektion-\\Matrix};
			\node[draw] (D) at (12,0) {Viewport-\\Matrix};
		\end{scope}
		
			\node (B1) at (6,0) {};
			\node (B2) at (6,-2) {Kamera-Koordinaten};
			
			\node (C1) at (10,0) {};
			\node (C2) at (10,-2) {NDC};
			
			\draw[->, line width=1mm] (A) edge (B);
			\draw[->, line width=1mm] (B) edge (C);
			\draw[->, line width=1mm] (C) edge (D);
			
			\draw[->] (B1) edge (B2);
			\draw[->] (C1) edge (C2);
	\end{tikzpicture}
	\centering
	\label{fig:Transformationspipeline}
	\caption{Die Abbildung zeigt die Transformationspipeline, mit deren Hilfe die Vertices
			 auf den Bildschirm abgebildet werden.}
\end{figure}

Die Eingabe der Transformationspipeline ist ein Vertex. Jeder Vertex besteht aus einer x-, y- und einer z-Komponente. 
Die Vertices sind im Modell-Koordinatensystem definiert, das bedeutet, dass der Ursprung
dieses Koordinatensystems das Zentrum des Modells ist.
Durch die Multiplikation mit der Modell-Matrix werden die Vertices in das Weltkoordinatensystem
projiziert.
Um die Koordinaten aus dem Weltkoordinatensystem in das Kamera-Koordinatensystem abzubilden genügt die Multiplikation mit der Kameramatrix.
In der Abbildung \ref{fig:Transformationspipeline} wurde die Modell- und die Kamera-Matrix
zur ModellKamera-Matrix zusammengefasst.
Werden die Vertices, die sich im Kamerakoordinatensystem befinden mit der Projektionsmatrix
multipliziert, dann werden diese in \textit{normalized-device-coordinates}, 
kurz NDC umgewandelt.
Normalisierte-Geräte-Koordinaten haben für die x-,y- und z-Komponente den Wertebereich von
$-1.0$ bis $1.0$. Durch die Multiplikation mit der Viewport-Matrix werden die x- und
 y-Komponente auf die Bildschirmkoordinaten abgebildet.

Um aus dem Tiefenbild normalisierte Gerätekoordinaten zu erhalten, müssen die x- und y-Koordinaten der Pixel durch die Auflösung geteilt werden.
Die Farbwerte des Tiefenbildes liegen im Bereich von $0.0$ bis $1.0$. Je größer der
Wert ist, umso weiter ist ein Pixel von der Bildschirmebene entfernt.
Die Farbwerte müssen ebenfalls in den Wertebereich von $-1.0$ bis $1.0$ überführt werden.

Damit aus den normalisierten Gerätekoordinaten wieder Modellkoordinaten werden,
reicht es aus diese mit der invertierten ModellKameraProjektionsmatrix zu multiplizieren.
Die daraus resultierenden Vertices, lassen sich erneut mit der Transformationspipeline
abbilden, auf diese Weise lassen sich die Koordinaten des einen Bildes in die eines
anderen überführen.

\section{Delaunay-Triangulierung}

Die Delaunay-Triangulierung ist ein Verfahren um ein Dreiecknetz aus einer Menge von
Punkten $p \in \mathbb{R}^{2}$ zu erzeugen.
Dabei wird für jedes Dreieck ein Umkreis erzeugt, innerhalb dessen keine Punkte eines anderen
Dreiecks enthalten sein dürfen.
Jedes Dreieck des zu erzeugenden Netzes muss diese Bedingung erfüllen.
Das Resultat dieser Forderung ist die maximierte Innenwinkelsumme aller Dreiecke.
Für eine gegebene Punktmenge, ist die Lösung nicht eindeutig, es kann verschiedene
Netzkonfigurationen geben, welche die Forderung erfüllen.

\section{PSNR}

Die Abkürzung PSNR in Englisch \textit{Peak signal-to-noise ratio}, gibt das Verhältnis
zwischen dem 

\textit{Peak signal-to-noise ratio}, kurz PSNR, gibt das Verhältnis zwischen dem Maximalwert und
der maximalen Störung an. 
Da die meisten Signale sehr große Skalen haben,
wird der PSNR häufig mittels einer logarithmischen Skala angeben.

Der PSNR wird zur Messung der Qualität von nicht verlustfreien Kompressionsalgorithmen verwendet.
Dazu wird das Originalbild als Signal interpretiert und der Fehler, der durch die Kompression
eingeführt wird als Rauschen.
Ein größer der PSNR-Wert bedeutet eine besser Qualität des dekomprimierten Bildes. 

Bei einer Farbtiefe von 8 Bit pro Kanal, stehen Werte von 30 - 40 dB für ein
geringes Störsignal.

Der \textit{mean squared error}, kurz $MSE$ summiert einem Fenster der Größe $m \times n$
die quadratischen Abstände zwischen dem Original und dem rekonstruierten Bild auf.

\begin{equation}
	MSE = \frac{1}{mn} \sum\limits_{i=0}^{m-1} \sum\limits_{j=0}^{n-1}[I(i, j) - K(i, j)]^2
\end{equation}

Beim PSNR wird der maximal mögliche Wert $MAX_I$ mit dem $MSE$ ins Verhältnis gesetzt:

\begin{align}
PSNR &= 10 \times \log_{10} \left( \frac{MAX^2_I}{MSE} \right) \\
	 &= 20 \times \log_{10} \left( \frac{MAX_I}{\sqrt{MSE}} \right) \\
	 &= 20 \times \log_{10} (MAX_I) - 10 \times \log_{10} (MSE)
\end{align}

\section{SSIM}

Eine weitere Metrik, die zum Vergleich von Bilder eingesetzt wird, wurde von Wang \textit{et al.}
entwickelt \cite{Wang04imagequality}.
Diese basiert auf der Idee das die Struktur der abgebildeten Objekte von Beleuchtung und
Kontrast unabhängig ist.

Beleuchtung und Kontrast können im gesamten Bild variieren, aus diesem Grund werden
beide Parameter lokal in einem Fenster bestimmt.

In ihrem System wird die Aufgabe, die Ähnlichkeit zu messen in drei Teile unterteilt:
Beleuchtung, Kontrast und Struktur.
Als Erstes wird die Beleuchtung bestimmt. 
Wenn die Signale, wie in diesem Fall diskret sind, lässt sich die mittlere Intensität, 
des Signals $x$ mit dem Term

\begin{equation}
	\mu_x = \frac{1}{N} \sum\limits_{i=1}^{N} x_i
\end{equation}

berechnen. Die Vergleichsfunktion für die Beleuchtung zwischen zwei Signalen $x$ und $y$
wird $l(x, y)$ und setzt die werte $\mu_x$ und $\mu_y$ wie folgt ins Verhältnis:

\begin{equation}
	l(x, y) = \frac{2\mu_x \mu_y + C_1}{\mu^2_x + \mu^2_y + C_1}.
\end{equation}

Als Nächstes wird die mittlere Intensität von den Signalen $x$ und $y$ abgezogen,
so dass $x' = x - \mu_x$ und $y' = y - \mu_y$ entstehen.

Der Kontrast des Signals $x'$ wird mit Hilfe der Standardabweichung $\sigma_{x'}$ approximiert.
In diskreter Form lässt sich diese mit der folgenden Formel berechnen:

\begin{equation}
	\sigma_{x'} = \left( \frac{1}{N-1} \sum\limits_{i=1}^{N} (x'_i - \mu_{x'})^{2} \right)^\frac{1}{2}.
\end{equation}

Die Vergleichsfunktion für den Kontrast wird mit $c(x, y)$ bezeichnet und definiert sich wie folgt:

\begin{equation}
	c(x, y) = \frac{2\sigma_x \sigma_y + C_2}{\sigma^2_x + \sigma^2_y + C_2}.
\end{equation}

An dieser Stelle wird das Signal normalisiert, 
indem es durch seine eigene Standartabweichung geteilt wird,
so dass die Signale $x'' = x' - \mu_{x'} / \sigma_x'$ und $y'' = y' - \mu_{y'} / \sigma_y'$ entstehen.
Der Vergleich der strukturellen Eigenschaften $s(x, y)$ wird mit den normalisierten Signalen $x''$
und $y''$ durchgeführt.
Für den Vergleich der Struktur sollen folgende Eigenschaften gelten:

Erstens, die Funktion $s(x, y)$ muss Symmetrisch sein $s(x, y) = s(y, x)$.

Zweitens soll die Funktion auf einen Wert kleiner oder gleich $1$ beschränkt werden $s(x, y) \le 1$.

Drittens es soll nur ein Maximum $s(x, y) = 1$, 
genau dann und nur dann, wenn gilt das $x = y$.

Eine Definition, die diese Forderungen erfüllt ist:

\begin{equation}
	s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x + \sigma_y + C_3}.
\end{equation}

Die Kovarianz $\sigma_{xy}$ berechnet sich folgendermaßen:

\begin{equation}
	\sigma_{xy} = \frac{1}{N-1} \sum\limits_{i=1}^{N} (x_i - \mu_x)(y_i - \mu_y).
\end{equation}

Sie korreliert mit dem Kosinus des Winkels zwischen den beiden Vektoren
$x - \mu_x$ und $y - \mu_y$.
Die Konstanten $C_1, C_2$ und $C_3$ in den drei Vergleichsfunktionen
sorgen dafür, das eine Division durch 0 nicht möglich wird, sollten
die Werte in den Nennern zu klein werden.
Letztlich kann die Gesamtqualität gemessen werden, indem Beleuchtung,
Kontrast und Strukturvergleich kombiniert werden:

\begin{equation}
	SSIM(x, y) = [l(x,y)]^\alpha \times [c(x, y)]^\beta \times [s(x, y)]^\gamma.
\end{equation}

Mit den Parametern $\alpha > 0, \beta > 0, \gamma > 0$, kann die
Gewichtung zwischen den Vergleichsfunktionen variiert werden.
Im Rahmen dieser Arbeit gilt $\alpha = \beta = \gamma = 1$ und
es gilt für die Konstanten:
$C_3 = C_2 / 2$, mit $C_1 = 6.5025$ und $C_2 = 58.5225$.
Der $SSIM$ wird lokal berechnet, in Fenstern der Größe $11 \times 11$,
dabei erden die Signalwerte $x_i$ und $y_i$ mit einer Gaussianfunktion
gewichtet.
Damit lassen sich die mittlere Intensität, die Standardabweichung
und die Kovarianz zu folgenden Gleichungen umschreiben, wobei $w_i$
die Gewichtung an dem Punkt $i$ bezeichnet:

\begin{align}
	\mu_x &= \sum\limits_{i=1}^{N} w_i x_i \\
	\sigma_x &= \left( \sum\limits_{i=1}^{N} w_i(x_i - \mu_x)^2 \right)^\frac{1}{2} \\
	\sigma_{xy} &= \sum\limits_{i=1}^{N} w_i(x_i - \mu_x)(y_i - \mu_y).
\end{align}

Die Gesamtgleichung für den $SSIM(x, y)$ lässt sich durch die Gleichung

\begin{equation}
	SSIM(x, y) = \frac{(2 \mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu^2_x + \mu^2_y + C_1)(\sigma^2_x + \sigma^2_y + C_2)}
\end{equation}

ausrechnen. Der $SSIM(x, y)$ misst die Güte jedoch nur lokal, um
eine Aussage über das gesamte Bild treffen zu können, kann der
Mittelwert über allen $x \in X$ und $y \in Y$ berechnet werden:

\begin{equation}
	MSSIM(X, Y) = \frac{1}{M} \sum\limits_{j=1}^{M} SSIM(x_j, y_j).
\end{equation}

\chapter{Methodik und Umsetzung}

Das Ziel dieser Arbeit ist es, ein Remote-Visualisierungssystem zu entwickeln
und eine Klient-seitige Bildextrapolation durchzuführen und die Güte der Extrapolation zu untersuchen und zu bewerten.
Zu diesen Zweck wird aus dem bei der Server-seitigen Bildsynthese entstehende Tiefenbild ein Dreiecksnetz erzeugt. 
Dieses wird vom Klient aus einer neuen Kameraperspektive gezeichnet, das Farbbild wird dabei als Textur verwendet.
Mit Hilfe von \textit{Ground-Truth} Szenen wird die Qualität, der extrapolierten Bildern
in Abhängigkeit der gewählten Parameter, mit dem PSNR und dem MSSIM gemessen.

Die Zentrale Aufgabe ist die Optimierung und die Übertragung der Tiefeninformationen,
vom Server zum Klient.
Es gibt zwei Möglichkeiten diese Aufgabe zu erfüllen.
Die erste Möglichkeit besteht darin das Tiefenbild direkt zu komprimieren,
zum Klient zu senden und die Konstruktion des Dreiecksnetzes auf von dem Klient durchführen zu lassen.
Die andere Möglichkeit besteht in der Konstruktion des Dreiecksnetzes auf dem Server,
vor der Kompression und Übertragung zum Klient durchzuführen.
In dieser Arbeit werden beide Varianten untersucht.

Als Grundlage dient das Konzept von Wessels \textit{et al.} \cite{DBLP:conf/itng/WesselsPJR11}.
Die Klient-Anwendung ist eine Browser basierte Web-Anwendung, die mit dem Server
über das WebSocket-Protokoll kommuniziert.
Dabei handelt es sich um ein auf TCP basierendes Netzwerkprotokoll,
das eine Bidirektionale Verbindung erlaubt.
Die Arbeit von Wessels \textit{et al.} wird dahingehend erweitert,
das die Manipulation der Bilddaten mit Hilfe von WebGl durchgeführt wird.
Bei WebGl handelt es sich um eine Bibliothek die von modernen Browsern zur Verfügung gestellt wird,
um eine Hardware beschleunigte Bildsynthese zu ermöglichen.
Der Vorteil dieser Technologie ist die Unabhängigkeit der Anwendung im Bezug,
zur Plattform und dem Gerät, auf dem sie laufen soll.

Im Folgenden wird die Konstruktion der Dreiecksnetze näher betrachtet.

\section{Erzeugung von Dreiecksnetzen}

Grundlage für die Erzeugung der Dreiecksnetze ist das Tiefenbild $D$.
Jeder Tiefenwert $d(x, y) \in D$ liegt in dem Intervall $[0, 1]$.

\subsection{Vollvernetzung}

Bei der Vollvernetzung kann das Dreiecksnetz vom Klient vor dem Erhalt des
ersten Bildes konstruiert werden.
Eine Neukonstruktion muss nur bei der Veränderung der Auflösung durchgeführt werden.
Jeder Pixel des Tiefenbildes entspricht dabei einem Eckpunkt im Dreiecksnetz.
Die z-Komponente jedes Eckpunktes wird initial mit 0 gesetzt.
Die Abbildungen \ref{fig:FULL_Default}, \ref{fig:FULL_Cookie} und \ref{fig:FULL_Isometric}
zeigen drei verschiedene Varianten der Vollvernetzung.

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[
			auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
			\node[vertice] (V0) at (0,0) {$v_0$};
			\node[vertice] (V1) at (2,0) {$v_1$};
			\node[vertice] (V2) at (4,0) {$v_2$};
			\node[vertice] (V3) at (6,0) {$v_3$};
			\node[vertice] (V4) at (8,0) {$v_4$};
			
			\node[vertice] (V5) at (0,-2) {$v_5$};
			\node[vertice] (V6) at (2,-2) {$v_6$};
			\node[vertice] (V7) at (4,-2) {$v_7$};
			\node[vertice] (V8) at (6,-2) {$v_8$};
			\node[vertice] (V9) at (8,-2) {$v_9$};
			
			\node[vertice] (V10) at (0,-4) {$v_{10}$};
			\node[vertice] (V11) at (2,-4) {$v_{11}$};
			\node[vertice] (V12) at (4,-4) {$v_{12}$};
			\node[vertice] (V13) at (6,-4) {$v_{13}$};
			\node[vertice] (V14) at (8,-4) {$v_{14}$};

			\node[vertice] (V15) at (0,-6) {$v_{15}$};
			\node[vertice] (V16) at (2,-6) {$v_{16}$};
			\node[vertice] (V17) at (4,-6) {$v_{17}$};
			\node[vertice] (V18) at (6,-6) {$v_{18}$};
			\node[vertice] (V19) at (8,-6) {$v_{19}$};
			
			\node[vertice] (V20) at (0,-8) {$v_{20}$};
			\node[vertice] (V21) at (2,-8) {$v_{21}$};
			\node[vertice] (V22) at (4,-8) {$v_{22}$};
			\node[vertice] (V23) at (6,-8) {$v_{23}$};
			\node[vertice] (V24) at (8,-8) {$v_{24}$};		
		\end{scope}
		
		\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V5); \draw[-] (V5) edge (V0);
		\draw[-] (V1) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V1);
		\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V6); \draw[-] (V6) edge (V1);
		\draw[-] (V2) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V2);
		\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V7); \draw[-] (V7) edge (V2);
		\draw[-] (V3) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V3);
		\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V8); \draw[-] (V8) edge (V3);
		\draw[-] (V4) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V4);
		
		\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
		\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
		\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V11);  \draw[-] (V11) edge (V6);
		\draw[-] (V7) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V7);
		\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
		\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
		\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V13);  \draw[-] (V13) edge (V8);
		\draw[-] (V9) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V9);

		\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V15); \draw[-] (V15) edge (V10);
		\draw[-] (V11) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V11);
		\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V16); \draw[-] (V16) edge (V11);
		\draw[-] (V12) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V12);
		\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V17); \draw[-] (V17) edge (V12);
		\draw[-] (V13) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V13);
		\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V18); \draw[-] (V18) edge (V13);
		\draw[-] (V14) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V14);
		
		\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V21); \draw[-] (V21) edge (V16);
		\draw[-] (V17) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V17);
		\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
		\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
		\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V23); \draw[-] (V23) edge (V18);
		\draw[-] (V19) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V19);	
	\end{tikzpicture}
	\centering
	\caption{Darstellung der Netzstruktur mit regulärer Vollvernetzung.}
	\label{fig:FULL_Default}
\end{figure}

Die Anzahl der Eckpunkte und der Textur-Koordinaten entspricht dabei der Anzahl der Bildpunkte des Tiefenbildes und die Anzahl an Dreiecken lässt sich aus der Auflösung $w \times h$ folgendermaßen berechnen: $2(w-1)(h-1)$.

Die Darstellung der Darstellungspipeline nach dem OpenGL ES 2.0 ist in Abbildung \ref{fig:OpenGl_pipeline} zu sehen. 
Mit gelb Farblich hervorgehoben sind die programmierbaren
Elemente, der Vertex- und der Fragment-Shader.

\subsection{16 Bit}

Farb- und Tiefenbild werden mit base64 kodiert übertragen und lassen
sich vom Browser nativ dekodieren.
Die Farbtiefe pro Kanal ist Browserseitig auf 8 Bit beschränkt.
Um 16 Bit Tiefeninformationen im Vertex-Shader nutzen zu können,
müssen diese 16 Bit auf zwei 8 Bit Kanäle aufgeteilt werden.
Die Abbildung \ref{fig:use2channels} verdeutlicht dieses Verfahren.
In den roten-Farbkanal werden die ersten 8 Bit und in den
grünen-Farbkanal die restlichen 8 Bit aufgeteilt.
Die Aufteilung der 16 Bit auf die Farbkanäle erfolgt Server-seitig.

\begin{figure}
	\begin{tikzpicture}
		\node[draw=none] (A) at (0,0) {$2^{16}$};
		\node[draw=none, fill=red!20, minimum width=6cm, minimum height=1cm] (B) at (-3,-1) {$2^{8}$};
		\node[draw=none, fill=green!20, minimum width=6cm, minimum height=1cm] (C) at (3,-1) {$2^{8}$};
		\node[draw=none] (D) at (-3,-2) {heigh};
		\node[draw=none] (E) at (3, -2) {low};
		
		\draw (-6, -2.5) -- (-6, 0.5) -- (6, 0.5) -- (6, -2.5);
		\draw (-6, -0.5) -- (6, -0.5);
		\draw (-6, -1.5) -- (6, -1.5);
		\draw (-6, -2.5) -- (6, -2.5);
		\draw (0, -2.5) -- (0, -0.5);
	\end{tikzpicture}
	\centering
	\caption{Hier wird die Aufteilung einer 16 Bit Zahl auf die Farbkanäle, links rot und rechts grün, visuell verdeutlicht. Das Schlüsselwort \textit{heigh} bezeichnet die ersten 8 Bit und \textit{low} die zweiten 8 Bit.}
	\label{fig:use2channels}
\end{figure}

Um aus \textit{heigh} und \textit{low} die 16 Bit $v$ zu berechnen, genügt es den Wert von \textit{heigh}
zuerst mit 255 zu multiplizieren und dann den Wert von \textit{low} zu addieren.
Die Gleichung \ref{eq:reconstruct1} zeigt genau diesen Zusammenhang:

\begin{equation}
	v = low + heigh \times 255.
	\label{eq:reconstruct1}
\end{equation}

Beim Laden einer Textur auf die Grafikkarte, werden alle Farbkanäle normiert, im Fall
eines 8 Bit Bildes, wird jeder Wert durch den Maximalwert 255 geteilt, so dass
der Wert im Intervall von $[0, 1]$ liegt.

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[
			auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
			\node[vertice] (V0) at (0,0) {$v_0$};
			\node[vertice] (V1) at (2,0) {$v_1$};
			\node[vertice] (V2) at (4,0) {$v_2$};
			\node[vertice] (V3) at (6,0) {$v_3$};
			\node[vertice] (V4) at (8,0) {$v_4$};
			
			\node[vertice] (V5) at (0,-2) {$v_5$};
			\node[vertice] (V6) at (2,-2) {$v_6$};
			\node[vertice] (V7) at (4,-2) {$v_7$};
			\node[vertice] (V8) at (6,-2) {$v_8$};
			\node[vertice] (V9) at (8,-2) {$v_9$};
			
			\node[vertice] (V10) at (0,-4) {$v_{10}$};
			\node[vertice] (V11) at (2,-4) {$v_{11}$};
			\node[vertice] (V12) at (4,-4) {$v_{12}$};
			\node[vertice] (V13) at (6,-4) {$v_{13}$};
			\node[vertice] (V14) at (8,-4) {$v_{14}$};

			\node[vertice] (V15) at (0,-6) {$v_{15}$};
			\node[vertice] (V16) at (2,-6) {$v_{16}$};
			\node[vertice] (V17) at (4,-6) {$v_{17}$};
			\node[vertice] (V18) at (6,-6) {$v_{18}$};
			\node[vertice] (V19) at (8,-6) {$v_{19}$};
			
			\node[vertice] (V20) at (0,-8) {$v_{20}$};
			\node[vertice] (V21) at (2,-8) {$v_{21}$};
			\node[vertice] (V22) at (4,-8) {$v_{22}$};
			\node[vertice] (V23) at (6,-8) {$v_{23}$};
			\node[vertice] (V24) at (8,-8) {$v_{24}$};		
		\end{scope}
		
		\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V6); \draw[-] (V6) edge (V0);
		\draw[-] (V0) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V0);
		\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V6); \draw[-] (V6) edge (V1);
		\draw[-] (V2) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V2);
		\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V8); \draw[-] (V8) edge (V2);
		\draw[-] (V2) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V2);
		\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V8); \draw[-] (V8) edge (V3);
		\draw[-] (V4) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V4);
		
		\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
		\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
		\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V12);  \draw[-] (V12) edge (V6);
		\draw[-] (V6) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V6);
		\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
		\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
		\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V14);  \draw[-] (V14) edge (V8);
		\draw[-] (V8) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V8);

		\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V16); \draw[-] (V16) edge (V10);
		\draw[-] (V10) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V10);
		\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V16); \draw[-] (V16) edge (V11);
		\draw[-] (V12) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V12);
		\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V18); \draw[-] (V18) edge (V12);
		\draw[-] (V12) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V12);
		\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V18); \draw[-] (V18) edge (V13);
		\draw[-] (V14) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V14);
		
		\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V22); \draw[-] (V22) edge (V16);
		\draw[-] (V16) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V16);
		\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
		\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
		\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V24); \draw[-] (V24) edge (V18);
		\draw[-] (V18) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V18);	
	\end{tikzpicture}
	\centering
	\caption{Darstellung der Netzstruktur mit \textit{cookie-cutter} Vollvernetzung.}
	\label{fig:FULL_Cookie}
\end{figure}

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[
			auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
			\node[vertice] (V0) at (0,0) {$v_0$};
			\node[vertice] (V1) at (2,0) {$v_1$};
			\node[vertice] (V2) at (4,0) {$v_2$};
			\node[vertice] (V3) at (6,0) {$v_3$};
			\node[vertice] (V4) at (8,0) {$v_4$};
			
			\node[vertice] (V5) at (0,-2) {$v_5$};
			\node[vertice] (V6) at (2,-2) {$v_6$};
			\node[vertice] (V7) at (4,-2) {$v_7$};
			\node[vertice] (V8) at (6,-2) {$v_8$};
			\node[vertice] (V9) at (8,-2) {$v_9$};
			
			\node[vertice] (V10) at (0,-4) {$v_{10}$};
			\node[vertice] (V11) at (2,-4) {$v_{11}$};
			\node[vertice] (V12) at (4,-4) {$v_{12}$};
			\node[vertice] (V13) at (6,-4) {$v_{13}$};
			\node[vertice] (V14) at (8,-4) {$v_{14}$};

			\node[vertice] (V15) at (0,-6) {$v_{15}$};
			\node[vertice] (V16) at (2,-6) {$v_{16}$};
			\node[vertice] (V17) at (4,-6) {$v_{17}$};
			\node[vertice] (V18) at (6,-6) {$v_{18}$};
			\node[vertice] (V19) at (8,-6) {$v_{19}$};
			
			\node[vertice] (V20) at (0,-8) {$v_{20}$};
			\node[vertice] (V21) at (2,-8) {$v_{21}$};
			\node[vertice] (V22) at (4,-8) {$v_{22}$};
			\node[vertice] (V23) at (6,-8) {$v_{23}$};
			\node[vertice] (V24) at (8,-8) {$v_{24}$};		
		\end{scope}
		
		\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V6); \draw[-] (V6) edge (V0);
		\draw[-] (V0) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V0);
		\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V7); \draw[-] (V7) edge (V1);
		\draw[-] (V1) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V1);
		\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V8); \draw[-] (V8) edge (V2);
		\draw[-] (V2) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V2);
		\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V9); \draw[-] (V9) edge (V3);
		\draw[-] (V3) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V3);
		
		\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
		\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
		\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V11);  \draw[-] (V11) edge (V6);
		\draw[-] (V7) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V7);
		\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
		\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
		\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V13);  \draw[-] (V13) edge (V8);
		\draw[-] (V9) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V9);

		\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V16); \draw[-] (V16) edge (V10);
		\draw[-] (V10) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V10);
		\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V17); \draw[-] (V17) edge (V11);
		\draw[-] (V11) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V11);
		\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V18); \draw[-] (V18) edge (V12);
		\draw[-] (V12) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V12);
		\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V19); \draw[-] (V19) edge (V13);
		\draw[-] (V13) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V13);
		
		\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V21); \draw[-] (V21) edge (V16);
		\draw[-] (V17) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V17);
		\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
		\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
		\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V23); \draw[-] (V23) edge (V18);
		\draw[-] (V19) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V19);	
	\end{tikzpicture}
	\centering
	\caption{Darstellung der Netzstruktur mit isometrischer Vollvernetzung.}
	\label{fig:FULL_Isometric}
\end{figure}

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[auto, every node/.style={minimum width=15em}]
		 
		\node[draw, fill=lightgray!20] (A) at (0,0) {Vertex Data};
		
		\node[draw, fill=yellow]	(B) at (0,-1) {Vertex Shader};
		
		\node[draw, fill=lightgray!20] (C) at (0, -2) {Primitive Assembly};

		\node[draw, fill=lightgray!20] (D) at (0,-3) {Rasterization};
		
		\node[draw, fill=yellow] 	(E) at (0,-4) {Fragment Shader};
		
		\node[draw, fill=lightgray!20] (F) at (0,-5) {Per-Fragment Operations};
		
		\node[draw, fill=lightgray!20] (G) at (0,-6) {Framebuffer};	

		\draw[->, line width=1mm] (A) edge (B);
		\draw[->, line width=1mm] (B) edge (C);
		\draw[->, line width=1mm] (C) edge (D);
		\draw[->, line width=1mm] (D) edge (E);
		\draw[->, line width=1mm] (E) edge (F);
		\draw[->, line width=1mm] (F) edge (G);
		
		\end{scope}
	\end{tikzpicture}
	\centering
	\caption{OpengGl ES 2.0 Darstellungspipeline. 
			 Grau unterlegt sind statischen und gelb die programmierbaren Elemente.}
	\label{fig:OpenGl_pipeline}
\end{figure}

\subsection{Delaunay-Triangulierung}

\chapter{Ergebnisse}

\chapter{Diskussion}

\chapter{Zusammenfassung}

\chapter{Ausblick}

\chapter{Noch mehr Ergebnisse}

\begin{tikzpicture}
\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in dB}, xlabel={in Grad},
			 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
			 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
\addplot[blue, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_4_512x512D10L0.0I0.0.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in dB}, xlabel={in Grad},
			 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
			 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
\addplot[blue, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_4_512x512D10L0.0I0.0.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in \%}, xlabel={in Grad},
			 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
			 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
\addplot[green, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_4_512x512D10L0.0I0.0.csv};
\addplot[red, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_4_512x512D10L0.0I0.0.csv};
\addplot[blue, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_4_512x512D10L0.0I0.0.csv};
\end{axis}
\end{tikzpicture}


\vspace*{1cm}

\begin{tikzpicture}
\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in \%}, xlabel={in Grad},
			 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
			 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
\addplot[red, only marks, mark=x] table [x=a, y=r, col sep=comma] {div_data_4_512x512D10L0.0I0.0.csv};
\addplot[green, only marks, mark=x] table [x=a, y=g, col sep=comma] {div_data_4_512x512D10L0.0I0.0.csv};
\addplot[blue, only marks, mark=x] table [x=a, y=b, col sep=comma] {div_data_4_512x512D10L0.0I0.0.csv};
\addplot[black, only marks, mark=x] table [x=a, y=m, col sep=comma] {div_data_4_512x512D10L0.0I0.0.csv};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={$t$ in $ms$}, xlabel={Bild Nr.},
			 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
			 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
\addplot[green] table [x=i, y=min, col sep=comma]{div_duration_info_4_512x512D10L0.0I0.0.csv};
\addplot[red] table [x=i, y=max, col sep=comma]{div_duration_info_4_512x512D10L0.0I0.0.csv};
\addplot[black] table [x=i, y=mean, col sep=comma]{div_duration_info_4_512x512D10L0.0I0.0.csv};
\addplot[blue, mark=x] table [x=i, y=d, col sep=comma]{div_duration_info_4_512x512D10L0.0I0.0.csv};
\end{axis}
\end{tikzpicture}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[view={0}{90}, 
		 			 grid=major,
		 			 xlabel=$T_{internal}$,
		 			 ylabel=$T_{leaf}$,
		 			 point meta min=0.7, point meta max=0.73,
		 			 colormap={greenyellow}{
		   			 	rgb255(0cm)	 =(128,0,0)
		   			 	rgb255(0.5cm) =(255,255,0) 
		   			 	rgb255(1cm) =(0,200,0)
		 			 }, 
		 			 colorbar, 
		 			 enlargelimits=true, 
		 			 xtick={0.0,0.1,...,1.1},
		     		 ytick={0.0,0.1,...,1.1},
		     		 xmajorgrids=true,
		     		 ymajorgrids=true]
		 
		 \addplot3 [only marks, 
		 		   mark=square*, 
		 		   mark size=7,
		 		   ycomb, 
		 		   scatter] file {div.csv};      
		\end{axis}
	\end{tikzpicture}
\end{figure}

\cite*{}
\end{document}