\documentclass[hyperref,german,diplominf]{cgvpub}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig

\usepackage{lmodern}
\pagenumbering{roman}
%\usepackage[ngerman]{babel}

\author{Josef Schulz}
\title{Optimierung und Übertragung von Tiefengeometrie für Remote-Visualisierung}
\birthday{20. Oktober 1989}
\placeofbirth{Naumburg (Saale)}
\matno{3658867}
\betreuer{Dr. Sebastian Grottel}
\bibfiles{literatur}

\problem{
In Big-Data-Szenarien in der Visualisierung spielt der Ansatz der Remote-Visualisierung eine zunehmende Rolle.  
Moderne Netzwerktechnologien bieten große Datenübertragungsraten und niedrige Latenzzeiten. Für die 
interaktive Visualisierung sind aber selbst kleinste Latenzzeiten problematisch. Um diese vor dem Benutzer maskieren zu können, kann eine Extrapolation der Darstellung durchgeführt. 
Diese Berechnungen erfordern zusätzlich zum normalen Farbbild weitere Daten, beispielsweise 
ein Tiefenbild und die Daten der verwendeten Kameraeinstellung.
Für die Darstellungsextrapolation werden Farb- und Tiefenbild zusammen interpretiert, beispielsweise als Punktwolke oder Höhenfeldgeometrie. 
Im Rahmen dieser Arbeit soll untersucht werden, wie die Darstellung mittels Höhenfeldgeometrie optimiert  werden kann. 
Ansätze sind hierfür Algorithmen aus der Netzvereinfachung. Zu erwarten sind sowohl 
harte Kanten als auch glatte Verläufe der Tiefenwerte, welche sich in der Netzgeometrie durch 
adaptive Vernetzung mit reduziertem Datenaufwand darstellen lassen.


Dem Szenario der Web-basierten Remote-Visualisierung folgend soll der Web-Browser als
Klient-Komponente eingesetzt werden. Die einzusetzenden Technologien sind HTML5, Javascript, 
WebGL und WebSockets. Entsprechende Javascript-Bibliotheken sollen genutzt 
werden um die Qualität und Wartbarkeit des Quellcodes zu steigern. Für die Server-Komponente darf die Technologie vom Bearbeiter frei gewählt werden.

Zu Beginn der Arbeit wird eine Literatur-Recherche zu Web-basierter Visualisierung und Remote-Visualisierung erfolgen. 
Schwerpunkte  sind hierbei  die  Bild-Extrapolation, Vernetzung 
und Rekonstruktion auf Basis von Tiefenbildern und die Netzoptimierung und -Vereinfachung. 
Im Anschluss an die Literaturrecherche wird ein Konzept für die Implementierung mit dem 
Betreuer abgesprochen und anschließend als prototypische Software umgesetzt. Folgendes 
Szenario dient als Grundlage für dieses Konzept:

Als  Eingabedaten  stehen  mehrere  Datensätze aus  unterschiedlichen  Szenarien  der  wissenschaftlichen Visualisierung zur Verfügung. Für jeden Datensatz sind mehrere Tripel aus Farbbild, Tiefenbild und Kamera-Parameter gegeben.
Die Serverkomponente bereitet einen Datensatz auf und bietet ihn dem Klienten an. Diese Aufbereitung ist vor allem die Generierung einer optimierten  Tiefennetzgeometrie  aus  den  Tiefenbilddaten.  Der  Klient  fordert  Farbbilder,  Kameraeinstellungen und Tiefengeometrie von Tripel-Paaren an.
Konzeptuell wird ein Tripel als aktueller Zustand und das zweite Tripel als Ground-Truth einer Bildextrapolation verstanden. 
Diese können daher auch in dieser Reihenfolge angefordert werden. 
Die Tripel werden zwischen  Klient  und  Server  direkt  per  Sockets/WebSockets  übertragen.
Die Daten des ersten Tripels werden anschließend genutzt um dessen Farbbild in die Ansicht des zweiten Tripels extrapoliert. Hierbei werden vom zweiten Tripel nur die Kameraeinstellung genutzt.
Diese Extrapolation wird  Klient-seitig in WebGL implementiert  damit  alle  Berechnungen  auf  der  GPU 
ausgeführt werden. 
Anschließend wird das extrapolierte Bild mit dem originalen Ground-Truth-Farbbild  aus  dem  zweiten  Tripel  verglichen  um  die  Qualität  der  Extrapolation  zu  bewerten, z.B. durch SSIM.

Die umgesetzte Lösung wird ausführlich evaluiert.
Zentraler Wert ist hierbei die Bildqualität nach der Extrapolation abhängig vom Winkelunterschied zwischen den Kameraeinstellungen und den Parametern der Vereinfachung der Tiefennetzgeometrie. 
Hierfür werden Tripel-Paare aus den Datensätzen und Variationen der Parameter der Algorithmen systematisch 
und automatisiert vermessen. Untersuchungen zum Laufzeitverhalten der Netzoptimierung im Server 
und der Bildextrapolation im Klienten sind optional durchzuführen.
}

\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}
\acknowledgments{Die Danksagung...}
\abstracten{abstract text english}
\abstractde{ Zusammenfassung Text Deutsch}

\usepackage{pgfplots}
%\usepackage{pgf-umlsd}
\usepackage{filecontents}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{float}
%\usepackage{setspace}

\setcounter{tocdepth}{2}

\input{../results/7/512x512_Full/default/16bit/5Pass/G1.0/results.tex}
\input{../results/6/512x512_Full/default/16bit/5Pass/G1.0/results.tex}

\input{../results/6/512x512_Delaunay/D10/L0.1/I0.1/results.tex}
\input{../results/7/512x512_Delaunay/D10/L0.1/I0.1/results.tex}


\pagenumbering{arabic}

%\newcommand{}[ANZAHL][OPTIONAL]{DEFINITION}

\begin{document}

\chapter{Einleitung}

Bei der Remote-Visualisierung, wird die Bildsynthese und die eigentliche Darstellung voneinander getrennt.
Der Server-Prozess erzeugt und kodiert jedes Bild zu einem kompakten Datenpaket, 
welches an den Klient-Prozess gesendet wird.
Der Klient empfängt und dekodiert das Datenpaket und gibt das Bild auf einem Bildschirm aus.

Remote-Visualisierung, ist ein insbesondere für mobile Endgeräte interessantes Konzept,
weil es die Visualisierung von komplexen Szenen auch auf Leistungsarmen Geräten ermöglicht.
Neben Computerspielen, ist die wissenschaftliche Visualisierung ein wichtiges Anwendungsgebiet,
da Datensätze Größenordnungen erreichen können, 
die den Speicher herkömmlicher Desktops, Laptops, Smartphones etc. bei weitem übersteigen. 
Auch wenn ausreichend Speicher zur Verfügung steht, 
kann die Übertragung dieser Daten viel Zeit in Anspruch nehmen.

Mit Hilfe der Remote-Visualisierung ist es möglich, dass der Server die Bildsynthese
übernimmt und nicht der komplette Datensatz übertragen werden muss.
Ein weiterer Vorteil der mit leistungsstarken Serversystemen einhergeht ist der, 
dass sich komplexe Visualisierungs- und Beleuchtungsmethoden verwenden lassen, die mit normalen Endgeräten nicht zu realisieren sind.

Die Latenz bezeichnet in der Netzwerktechnik die Übertragungszeit von einem zum anderen Gerät.
Diese ist in modernen Netzwerken gering, für die Interaktive Visualisierung allerdings immer
noch zu groß um eine für den Menschen nicht wahrnehmbare Verzögerungszeit und 
damit eine interaktive Visualisierung zu gewährleisten.
Ein möglicher Ausweg besteht darin, dass der Klient-Prozess ein bereits empfangenes Bild extrapoliert. 
Auf diese Weise, ist es möglich, dass der Klient-Prozess bereits ein neues Bild ausgeben kann, 
obwohl es noch nicht empfangen wurde.

Bei der Bildsynthese des Server-Prozesses entsteht neben dem Farbbild zusätzlich ein Tiefenbild.
Dieses kann genutzt werden, um geometrische Informationen an den Klient-Prozess weiter zureichen.
Geometrisch entspricht das Tiefenbild einer 2.5D Ansicht der Szene, in Form einer Punktwolke.
Die Extrapolation eines Bildes wird durchgeführt, indem diese Informationen aus einer
neuen Kameraperspektive gezeichnet wird.
Um die Qualität zu verbessern und Informationen einzusparen, wird aus dem Tiefenbild
ein Dreiecksnetz erzeugt.
Das Farbbild wird dabei als Textur über das Netz gelegt.

Die Zentrale Aufgabe ist die Optimierung und die Übertragung der Tiefeninformationen,
vom Server zum Klient.
Es gibt im wesentlichen zwei Möglichkeiten diese Aufgabe zu erfüllen.
Zum einen, kann das Tiefenbild direkt komprimiert werden 
und im Anschluss daran wird es an den Klient gesendet,
welcher das Dreiecksnetz aus dem Tiefenbild erzeugt und die Extrapolation durchführt.
Die zweite Möglichkeit besteht in der Konstruktion des Dreiecksnetzes durch den Server.
Beide Varianten werden in dieser Arbeit untersucht und mit Ground-Truth Datensätzen, hinsichtlich ihrer Güte in Abhängigkeit des Kamerawinkels evaluiert. 
Die Abbildung \ref{fig:datasets_frame0} zeigt zwei Bilder aus den Datensätzen.
Um die Algorithmen zu testen wurde eine Server- und eine Klient-Komponente entwickelt.
Beide Komponenten tauschen Informationen über das auf TCP basierende Websocket-Protokoll aus.
Der Klient ist ein Browser basierter Web-Klient, damit die implementierten Algorithmen
mit den Einschränkungen durch JavaScript und WebGl evaluiert werden können.

Im Folgenden werden bereits existierender Arbeiten diskutiert.

\begin{figure}[h]
	\subfigure[CoolRandom]{\includegraphics[width=.49\textwidth]{../Scenes/CoolRandom/run_1/coolrandom_00000.png}}
	\subfigure[TestSpheres]{\includegraphics[width=.49\textwidth]{../Scenes/TestSpheres/run_1/testspheres_00000.png}}
\caption{Die Abbildungen a und b zeigen Bilder aus den Datensätzen.}
\label{fig:firstframes}
\end{figure}

Das Ziel dieser Arbeit ist es, ein Remote-Visualisierungssystem zu entwickeln,
und eine Klient-seitige Bildextrapolation durchzuführen.
Grundlage sind jeweils ein Farbbild, ein Tiefenbild und die entsprechenden Kamerainformationen.

\chapter{Verwandte Arbeiten}

Einen Überblick über Architekturen und Methoden der interaktiven Remote-Visualisierung geben Shu Shi \textit{et al.} \cite{Shi:2015:SIR:2775083.2719921}.
Zum zentralen Problem ihrer Arbeit, wird die Latenz und die effiziente Übertragung der Daten vom Server zum Klient.
Lösungen hängen vom Anwendungsfall ab.
In \textit{THIN}-Systemen besteht die Aufgabe in der Übertragung von
2D Informationen, mit denen sich zum Beispiel Desktop-Anwendungen Fernsteuern lassen.
Beispiele sind \textit{SLIM} \cite{Schmidt:1999:IPS:319344.319154} und \textit{THiNC} \cite{Baratto:2005:TVD:1095809.1095837}. Sie sind für die Übertragung von 2D-Daten optimiert und profitieren davon, dass die meisten Änderungen nur Teilbereiche der eigentlichen Oberfläche betreffen.
Ein anderes extrem der Remote-Visualisierung ist die Aufteilung der Bildsynthese,
auf verschiedene Host-Systeme wie bei WireGL \cite{Humphreys:2001:WSG:383259.383272}.
Jin Zhefan stellt in diesem Kontext Kompressionsmethoden für Zeicheninstruktionen, Vektoren, Normalen und Texturinformationen vor \cite{Jin:2006:RRI:1128923.1128927}.
Peter Eisert und Philipp Fechteler haben ein Remote-Visualisierungssystem für Computerspiele entwickelt \cite{Eisert07remoterendering}.
Ihr System überträgt bei kleinen Auflösungen, für Endgeräte ohne GPU die Bilder
kodiert mit h264 oder wahlweise mpeg4.
Bei größeren Auflösungen werden Zeicheninstruktionen gesendet und die Bilder werden
vom Klienten synthetisiert.
Auch wenn ihr System ausschließlich für den Einsatz in lokalen Netzwerken konzipiert wurde,
ist die Latenz zu hoch um vom Benutzer nicht registriert zu werden.
Mit Hilfe einer Bildextrapolation durch den Klienten kann die Latenz reduziert werden. 
Zu diesen Zweck schätzen Shu \textit{et al.} in ihrer Arbeit \cite{1331221}, 
zusätzliche Referenz-Kamerapositionen und erzeugen zusätzlich zum Farbbild der Originalen
Kameraposition weitere Tiefenbilder. 
Das Farbbild wird mit JPEG komprimiert und die Tiefenbilder mit ZLIB.
Mit Hilfe der zusätzlichen Tiefeninformationen können durch Verdeckung bedingte Lücken
bei der Extrapolation durch den Klienten geschlossen werden.
Ein ähnlicher Ansatz wurde im VR-Bereich von Smit \textit{et al.} erprobt \cite{4810995}.
Auch in diesem System werden Lücken mit Hilfe von Tiefenbildern aus Referenz-Kamerapositionen
geschlossen.
Die Wahl des Warping-Algorithmus ist entscheidend für die Performance der Klient-Komponente,
neben der im VR-System zum Einsatz kommenden Variante \cite{Mark:1997:PW:253284.253292},
bieten Mark \textit{et al.} eine Übersicht über verschiedene mögliche Verfahren \cite{Mark99post-rendering3d}.

Die Übertragung und Visualisierung von Tiefenbildern spielt auch in der Visualisierung 
von Sensordaten eine wichtige Rolle.
Debevec1998
Palomo \textit{et al.} beschäftigen sich mit der Visualisierung von Tiefenbildern \cite{Palomo:2010:EAD:1900179.1900236}, in ihrem Fall wurden diese mit Tiefensensoren erhoben.
Sie kombinieren mehrerer aus verschiedenen Blickwinkeln aufgenommenen Bilder mit Hilfe
eines Tiefenpuffers zu einem Gesamtbild und vermindern auf diese Weise
entstehende Artefakte im Zusammenhang von Verdeckung bei der Visualisierung der Daten aus einer
anderen Perspektive.

Um die Latenz bei der Übertragung von Tiefeninformation zu verbessern,
können die Daten auch progressiv zum Klient gesendet werden.
Evans \textit{et al.} haben für diesen Zweck ein Datenformat für Punktwolken entwickelt und für den Einsatz mit WebGL optimiert \cite{Evans:2014:WVO:2668904.2668937}.
 
Banno \textit{et al.} erzeugen mit Hilfe der Delaunay-Triangulierung adaptive
Dreiecksnetze aus Tiefenbildern, um die Qualität der Darstellung zu verbessern \cite{Banno:2012:RCD:2407516.2407579}.

Lee \textit{et al.} 

Eine Vergleichbaren Ansatz wird von untersucht.



Wessels \textit{et al.} stellen eine Konzeption für den Programmaufbau eines interaktiven Remote-Visualisierungssystem basierend auf dem WebSocket-Protokoll vor \cite{DBLP:conf/itng/WesselsPJR11}.
In ihrem System besteht der Server-Prozess aus zwei Hauptkomponenten,
der Visualisierungs-Engine und dem Deamon. 
Während die Visualisierungs-Engine für die Bildsynthese zuständig ist, 
übernimmt der Deamon die Kommunikation mit dem Klient-Prozess.
Der Klient schickt dabei seine Eingabeinformationen von Maus und Tastatur direkt an den Server.
Dieser wertet die Daten aus und erzeugt darauf hin ein mit JPEG komprimiertes Bild, das mit Base64 kodiert wird und schließlich an den Klient-Prozess geschickt wird. 
Dieser kann das Bild nativ mit Hilfe eines HTML5 Canvas dekodieren und darstellen.
Ihr System wird zur Grundlage dieser Arbeit.



Das erzeugte Tiefenbild lässt sich 
Pauly et al. \cite{Pauly:2002:ESP:602099.602123}

Simon Stegmaier \cite{Stegmaier02ageneric} 
A Generic Solution for Hardware-Accelerated Remote Visualization

Gabriel Taubin und Jarek Rossignac haben ein Algorithmus zur Erzeugung und effizienten Kodierung von Dreiecksstreifen aus Dreiecksnetzen entwickelt \cite{Taubin:1998:GCT:274363.274365}.
Dazu konstruiert ihr Algorithmus Spannbäume über dem Netz, die zur Erzeugung möglichst großer
Dreiecksstreifen genutzt werden. Die Kompression kann wahlweise verlustfrei oder verlustbehaftet
durchgeführt werden. Typische Kompressionsraten werden mit 1:50 angegeben.

Eine weitere Arbeit die sich mit der Kompression von Dreiecksnetzen und einer kompakten Repräsentation
von diesen beschäftigt wurde Stefan Gumhold und Wolfgang Straßer geschrieben \cite{Gumhold:1998:RTC:280814.280836}.
Kompression und Dekompression sind echtzeitfähig.

Michael Deering hat ebenfalls ein Geometrisches Kompressionsverfahren entwickelt. \cite{Deering:1995:GC:218380.218391}

Federico Ponchio  und Matteo Dellepiane \cite{Ponchio:2015:FDW:2775292.2775308}
Fast decompression for web-based view-dependent 3D rendering



Diplomarbeit mädcheninformatiker Effiziente Datenübertragung von Modellen und
Texturen für die Verwendung in WebGL
Stefan Wagner noch kein cite



WEB-BASED VISUALISATION OF ON-SET POINT CLOUD
DATA Alun Evans et al \cite{Evans:2014:WVO:2668904.2668937}

planarer scheiß \cite{Ma13ecmr}



\chapter{Grundlagen}

In diesem Kapitel werden die Datensätze im Detail vorgestellt und es wird gezeigt,
wie die Bildextrapolation anhand der zur Verfügung stehenden Informationen 
durchgeführt wird.
Anschließend werden die zwei Metriken PSNR und SSIM zum Vergleich von Bildern vorgestellt,
die für die Evaluation verwendet werden.

\section{Datensätze}

Zur Analyse der verwendeten Methoden stehen zwei Datensätze zur Verfügung.
Jeder Datensatz besteht aus einer Sequenz von Tripeln.
Dabei setzt sich jedes Tripel aus einem Farbbild $I$, einem Tiefenbild $D$ 
sowie einem Satz von Kameraparametern zusammen.
Die Auflösung der Farb- und Tiefenbilder beträgt für alle Datensätze $w \times h = 512 \times 512$.
Die Tiefenbilder wurden mit 16 Bit und die Farbbilder mit 24 Bit quantisiert, dabei entfallen jeweils
8 Bit auf die einzelnen Farbkanäle.
In der Abbildung \ref{fig:testspheres_40_id} werden Farb- und Tiefenbild des ersten Tripels aus dem Datensatz \textit{TestSpheres} dargestellt.

\begin{figure}[h]
	\subfigure[]{\includegraphics[width=.49\textwidth]{../Scenes/TestSpheres/run_2/testspheres_00000.png}}
	\subfigure[]{\includegraphics[width=.49\textwidth]{../Scenes/TestSpheres/run_2/testspheres_00000_depth.png}}
	\caption{Farb und Tiefenbild aus dem Datensatz \textit{TestSpheres}}
	\label{fig:testspheres_40_id}
\end{figure}

Die Kameraparameter setzen sich aus einem Vektor für die Kameraposition, einem \textit{lookAt}-Vektor, der Punkt, auf den die Kamera schaut und dem \textit{up}-Vektor zusammen.
Für die Projektion werden die Positionen der Clippingebenen durch die Werte $z_{near}$ und $z_{far}$ beschrieben.
Des Weiteren wird der Öffnungswinkel der Kamera benötigt.

Von beiden Szenen stehen 493 Bilder zur Verfügung.
Beide Datensätze lassen sich in Abhängigkeit der Winkelschrittweite in 3 Sequenzen unterteilen.
Der Winkel gibt dabei den Unterschied zwischen der Kameraposition aus dem ersten und dem gerade betrachteten Frame an.
Eine Auflistung der Sequenzen zeigt die Tabelle \ref{tab:datasets}.

\begin{table}[h]
	\begin{tabular}{c|c|c|c|c}
		\# & Anzahl Bilder & min Winkel &  max Winkel & Winkel Schritt \\
		\hline
		1 & 241 & 0  & 5  & 0.25 \\
		2 & 60  & 6  & 10 & 1 	 \\
		3 & 192 & 15 & 90 & 5  	 \\
	\end{tabular}
	\centering
	\caption{Die Tabelle zeigt eine Übersicht über die Winkel der aufgenommenen Sequenzen.}
	\label{tab:datasets}
\end{table}

Die Datensätze \textit{CoolRandom} und \textit{TestSpheres} wurden aus Partikeldatensätzen erzeugt.
\textit{TestSpheres} ist ein synthetisch erzeugter Datensatz, der aus wenigen Partikeln geniert wurde.
Bei dem Datensatz \textit{CoolRandom} handelt es sich dagegen um einen komplexen Datensatz,
der mit Molekül-Datensätzen vergleichbar ist, welche in existierenden Anwendungen visualisiert werden.

\section{Extrapolation}

Eine einfache Möglichkeit, um die Extrapolation durchzuführen, besteht darin, das Tiefenbild als Punktwolke
zu interpretieren und diese aus einer neuen Kameraperspektive zu zeichnen.
Dazu wird aus jedem Pixel $d \in D$ ein Vertex erzeugt. 
Mit Hilfe der Rückprojektion lassen sich die Vertices in die gewünschten Positionen transformieren.

Um das Prinzip der Rücktransformation zu erläutern, soll zunächst betrachtet werden, wie ein Vertex $v$
aus dem Modellkoordinatensystem in normalisierte Gerätekoordinaten transformiert wird.
Die Abbildung \ref{fig:Transformation} verdeutlicht diesen Vorgang. 

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[auto, every node/.style={minimum size=7em, align=center}]
			\node[draw] (A) at (0,0) {Modell-\\koordinaten};
			\node[draw] (B) at (4,0) {Welt-\\koordinaten};
			\node[draw] (C) at (8,0) {Kamera-\\koordinaten};
			\node[draw] (D) at (12,0) {normalisierte \\Geräte- \\koordinaten};
		\end{scope}
		\draw[->, gray, line width=1mm] (A) -- (B);
		\draw[->, gray, line width=1mm] (B) -- (C);
		\draw[->, gray, line width=1mm] (C) -- (D);
	\end{tikzpicture}
	\centering
	\caption{Die Abbildung zeigt die Teilschritte der Transformation eines Vertices aus den Modellkoordinaten in Bildschirmkoordinaten.}
	\label{fig:Transformation}
\end{figure}

Der Vertex $v = (x, y, z, 1)^T$ liegt zunächst in homogenen Modellkoordinaten vor.
Mit Hilfe der Modellmatrix $M = RT$, die aus einer Rotations- und einer Translationsmatrix besteht, 
lässt sich der Vektor in das Weltkoordinatensystem überführen.
Die Transformation in das Kamerakoordinatensystem wird durch die Multiplikation der Kameramatrix $V$ mit dem Vekotr $v$ berechnet.
Schließlich kann die Projektion aus dem Kamerakoordinatensystem in normalisierte Gerätekoordinaten mit Hilfe
der Projektionsmatrix $P$ berechnet werden.
In der Gleichung \ref{eq:ModelViewProjektion} wird die Transformation von dem Vertex $v$ aus
den Modellkoordinaten in die normalisierten Gerätekoordinaten $v'$ zusammengefasst:

\begin{equation}
	v' = M \cdot V \cdot P \cdot v.
	\label{eq:ModelViewProjektion}
\end{equation}

Normalisierte Gerätekoordinaten haben für die x-,y- und z-Komponente den Wertebereich von
$-1$ bis $1$. 
Diese lassen sich in Bildschirmkoordinaten umrechnen, indem die drei Komponenten in das
Intervall $[0, 1]$ übersetzt werden.
Anschließend werden die x- und die y-Komponenten
auf den Bildbereich gestreckt, indem sie mit $w-1$ und $h-1$ der gewünschten Auflösung
$w \times h$ multipliziert werden. 

Im Folgenden wird die Menge der Tiefenbilder $D_i$ und den dazugehörigen Transformationsmatrizen $T_i$, mit $i \in \{0,1, ..., N \}$ betrachtet.
Jede Transformatiosmatrix $T_i$ setzt sich dabei aus einer Modellmatrix $M_i$, einer Kameramatrix $V_i$ und einer
Projektionsmatrix $P_i$ zusammen:

\begin{equation}
	T_i = M_i \cdot V_i \cdot P_i.
\end{equation}

Um die Punktwolke des Tiefenbildes $D_i$ mit der Transformationsmatrix $T_j$ auf das Tiefenbild $D_j$
abzubilden, muss zunächst für jeden Pixel $d_i \in D_i$ ein Vertex $v_i$ erzeugt werden, mit $i,j \in \{0,1, ..., N \}$.
Zunächst liegt $v_i$ in Bildschirmkoordinaten des Pixels $d_i$ vor und muss in normalisierte Gerätekoordinaten übersetzt werden. 
Zu diesem Zweck werden zuerst die x- und y-Kompontenten durch $w-1$ beziehungsweise $h-1$ geteilt.
Anschließend müssen alle drei Komponenten des Vektors $v_i$ aus dem Werteintervall $[0, 1]$
in das Intervall $[-1, 1]$ transformiert werden.
Jetzt liegt $v_i$ in normalisierten Gerätekoordinaten vor.
Durch die Multiplikation der inversen Transformationsmatrix $T_i^{-1}$ mit $v_i$, 
kann der Vertex in seine Modellkoordinaten überführt und
im Anschluss mit Hilfe von $T_j$
aus einer neuen Kameraperspektive gezeichnet werden:

\begin{equation}
	v_i' = T_j \cdot T_i^{-1} v_i.
\end{equation}

Die Interpretation der Tiefenbilder als Punktwolke ist keine optimale Lösung,
da bei der Bildsynthese Lücken im Bild entstehen.
Eine bessere Lösung ist es, aus dem Tiefenbild ein Dreiecksnetz zu erzeugen
und dieses für die Bildsynthese zu verwenden.
Mit Hilfe der Delaunay-Triangulierung ist es möglich aus Tiefenbildern
adaptive Dreiecksnetze zu erzeugen, welche die Lücke füllen.

\section{Delaunay-Triangulierung}

Die Delaunay-Triangulierung ist ein Verfahren, um ein Dreiecknetz aus einer Menge von
Punkten $p \in \mathbb{R}^{2}$ zu erzeugen.
Dabei wird für jedes Dreieck ein Umkreis erzeugt, innerhalb dessen keine Punkte eines anderen
Dreiecks enthalten sein dürfen.
Jedes Dreieck des zu erzeugenden Netzes muss diese Bedingung erfüllen.
Das Resultat dieser Forderung ist die maximierte Innenwinkelsumme aller Dreiecke.
Für eine gegebene Punktmenge ist die Lösung nicht eindeutig, es kann verschiedene
Netzkonfigurationen geben, welche die Forderung erfüllen.

Es existieren verschiedene Algorithmen die Delaunay-Triangulierung durchzuführen.
Die besten erreichen eine Laufzeit von $O(n \log n)$ und sind damit
für den Einsatz in Echtzeitanwendungen tauglich.
Beispiele sind der Sweep-Algorithmus und die inkrementelle Konstruktion.

\section{PSNR}

Die Abkürzung PSNR in Englisch \textit{Peak signal-to-noise ratio}, gibt das Verhältnis
zwischen dem 

\textit{Peak signal-to-noise ratio}, kurz PSNR, gibt das Verhältnis zwischen dem Maximalwert und
der maximalen Störung an. 
Da die meisten Signale sehr große Skalen haben,
wird der PSNR häufig mittels einer logarithmischen Skala angeben.

Der PSNR wird zur Messung der Qualität von nicht verlustfreien Kompressionsalgorithmen verwendet.
Dazu wird das Originalbild als Signal interpretiert und der Fehler, der durch die Kompression
eingeführt wird als Rauschen.
Ein größer der PSNR-Wert bedeutet eine besser Qualität des dekomprimierten Bildes. 

Bei einer Farbtiefe von 8 Bit pro Kanal, stehen Werte von 30 - 40 dB für ein
geringes Störsignal.

Der \textit{mean squared error}, kurz $MSE$ summiert einem Fenster der Größe $m \times n$
die quadratischen Abstände zwischen dem Original und dem rekonstruierten Bild auf.

\begin{equation}
	MSE = \frac{1}{mn} \sum\limits_{i=0}^{m-1} \sum\limits_{j=0}^{n-1}[I(i, j) - K(i, j)]^2
\end{equation}

Beim PSNR wird der maximal mögliche Wert $MAX_I$ mit dem $MSE$ ins Verhältnis gesetzt:

\begin{align}
PSNR &= 10 \times \log_{10} \left( \frac{MAX^2_I}{MSE} \right) \\
	 &= 20 \times \log_{10} \left( \frac{MAX_I}{\sqrt{MSE}} \right) \\
	 &= 20 \times \log_{10} (MAX_I) - 10 \times \log_{10} (MSE)
\end{align}

\section{SSIM}

Eine weitere Metrik, die zum Vergleich von Bilder eingesetzt wird, wurde von Wang \textit{et al.}
entwickelt \cite{Wang04imagequality}.
Diese basiert auf der Idee das die Struktur der abgebildeten Objekte von Beleuchtung und
Kontrast unabhängig ist.

Beleuchtung und Kontrast können im gesamten Bild variieren, aus diesem Grund werden
beide Parameter lokal in einem Fenster bestimmt.

In ihrem System wird die Aufgabe, die Ähnlichkeit zu messen in drei Teile unterteilt:
Beleuchtung, Kontrast und Struktur.
Als Erstes wird die Beleuchtung bestimmt. 
Wenn die Signale, wie in diesem Fall diskret sind, lässt sich die mittlere Intensität, 
des Signals $x$ mit dem Term

\begin{equation}
	\mu_x = \frac{1}{N} \sum\limits_{i=1}^{N} x_i
\end{equation}

berechnen. Die Vergleichsfunktion für die Beleuchtung zwischen zwei Signalen $x$ und $y$
wird $l(x, y)$ und setzt die werte $\mu_x$ und $\mu_y$ wie folgt ins Verhältnis:

\begin{equation}
	l(x, y) = \frac{2\mu_x \mu_y + C_1}{\mu^2_x + \mu^2_y + C_1}.
\end{equation}

Als Nächstes wird die mittlere Intensität von den Signalen $x$ und $y$ abgezogen,
so dass $x' = x - \mu_x$ und $y' = y - \mu_y$ entstehen.

Der Kontrast des Signals $x$ wird mit Hilfe der Standardabweichung $\sigma_{x}$ approximiert.
In diskreter Form lässt sich diese mit der folgenden Formel berechnen:

\begin{equation}
	\sigma_{x} = \left( \frac{1}{N-1} \sum\limits_{i=1}^{N} (x_i - \mu_{x})^{2} \right)^\frac{1}{2}.
\end{equation}

Die Vergleichsfunktion für den Kontrast wird mit $c(x, y)$ bezeichnet und definiert sich wie folgt:

\begin{equation}
	c(x, y) = \frac{2\sigma_x \sigma_y + C_2}{\sigma^2_x + \sigma^2_y + C_2}.
\end{equation}

An dieser Stelle wird das Signal normalisiert, 
indem es durch seine eigene Standartabweichung geteilt wird,
so dass die Signale $x'' = x' - \mu_{x'} / \sigma_x'$ und $y'' = y' - \mu_{y'} / \sigma_y'$ entstehen.
Der Vergleich der strukturellen Eigenschaften $s(x, y)$ wird mit den normalisierten Signalen $x''$
und $y''$ durchgeführt.
Für den Vergleich der Struktur sollen folgende Eigenschaften gelten:

\begin{figure}
	\begin{tikzpicture}[scale=0.7]
			\begin{scope}[auto, every node/.style={}]
				\node[] 										(A) at (0,0) 		{\small Signal $x$};
				\node[draw, circle] 							(B) at (5,-1.5) 	{\small $+$};
				\node[draw, circle] 							(C) at (10,-3) 		{\small $\div$};
				\node[draw, align=center, minimum width=2cm] 	(D) at (3,0) 		{\small Luminanz \\ Messung};
				\node[draw, align=center, minimum width=2cm] 	(E) at (7.5,-1.5) 	{\small Kontrast \\ Messung};
				\node[draw, align=center, minimum width=2cm] 	(F) at (13,-1.5) 	{\small Luminanz \\ Messung};
				\node[draw, align=center, minimum width=2cm] 	(G) at (13,-4) 		{\small Kontrast \\ Messung};
				\node[draw, align=center, minimum width=2cm] 	(H) at (13,-6.5) 	{\small Struktur \\ Messung};
				\node[draw, align=center, minimum height=1.3cm] (I) at (16.5,-4) 	{\small Kombination};
				\node[align=center] 							(J) at (20,-4) 		{\small Ähnlichkeits \\ Messung};
				\node[] 										(K) at (0,-5) 		{\small Signal $y$};
				\node[draw, circle] 							(L) at (5,-6.5) 	{\small $+$};
				\node[draw, circle] 							(M) at (10,-8) 		{\small $\div$};
				\node[draw, align=center, minimum width=2cm] 	(N) at (3,-5) 		{\small Luminanz \\ Messung};
				\node[draw, align=center, minimum width=2cm] 	(O) at (7.5,-6.5) 	{\small Kontrast \\ Messung};
			\end{scope}
			
			\draw[->, thick] (A) -- (D);
			\draw[->, thick] (D) -- (11, 0) -- (F.west);
			\draw[->, thick] (1.2, 0) |- (B) 	node[inner sep=8pt, anchor=north east] {+};
			\draw[->, thick] (5, 0) -- (B) 		node[inner sep=8pt, anchor=south west] {-};
			\draw[->, thick] (B) -- (E);
			\draw[->, thick] (E) -- (11, -1.5) -- (G.west);
			\draw[->, thick] (5.7, -1.5) |- (C);
			\draw[->, thick] (10, -1.5) -- (C);
			\draw[->, thick] (C) -- (11, -3) -- (H.west);
			\draw[->, thick] (K) -- (N);
			\draw[->, thick] (N) -- (11, -5) -- (F.west);
			\draw[->, thick] (1.2, -5) |- (L) 	node[inner sep=8pt, anchor=north east] {+};
			\draw[->, thick] (5, -5) -- (L) 	node[inner sep=8pt, anchor=south west] {-};
			\draw[->, thick] (L) -- (O);
			\draw[->, thick] (O) -- (11, -6.5) -- (G.west);
			\draw[->, thick] (5.7, -6.5) |- (M);
			\draw[->, thick] (10, -6.5) -- (M);
			\draw[->, thick] (M) -- (11, -8) -- (H.west);			
			\draw[->, thick] (G) -- (I);
			\draw[->, thick] (F) -| (I.north);
			\draw[->, thick] (H) -| (I.south);
			\draw[->, thick] (I) -- (J);
	\end{tikzpicture}
	\centering
	\caption{Das Modell zeigt das Vorgehen von dem SSIM-Algorithmus}
	\label{fig:SSIM_DIAGRAM}
\end{figure}

Erstens, die Funktion $s(x, y)$ muss Symmetrisch sein $s(x, y) = s(y, x)$.

Zweitens soll die Funktion auf einen Wert kleiner oder gleich $1$ beschränkt werden $s(x, y) \le 1$.

Drittens es soll nur ein Maximum $s(x, y) = 1$, 
genau dann und nur dann, wenn gilt das $x = y$.

Eine Definition, die diese Forderungen erfüllt ist:

\begin{equation}
	s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x + \sigma_y + C_3}.
\end{equation}

Die Kovarianz $\sigma_{xy}$ berechnet sich folgendermaßen:

\begin{equation}
	\sigma_{xy} = \frac{1}{N-1} \sum\limits_{i=1}^{N} (x_i - \mu_x)(y_i - \mu_y).
\end{equation}

Sie korreliert mit dem Kosinus des Winkels zwischen den beiden Vektoren
$x - \mu_x$ und $y - \mu_y$.
Die Konstanten $C_1, C_2$ und $C_3$ in den drei Vergleichsfunktionen
sorgen dafür, das eine Division durch 0 nicht möglich wird, sollten
die Werte in den Nennern zu klein werden.
Letztlich kann die Gesamtqualität gemessen werden, indem Beleuchtung,
Kontrast und Strukturvergleich kombiniert werden:

\begin{equation}
	SSIM(x, y) = [l(x,y)]^\alpha \times [c(x, y)]^\beta \times [s(x, y)]^\gamma.
\end{equation}

Mit den Parametern $\alpha > 0, \beta > 0, \gamma > 0$, kann die
Gewichtung zwischen den Vergleichsfunktionen variiert werden.
Im Rahmen dieser Arbeit gilt $\alpha = \beta = \gamma = 1$ und
es gilt für die Konstanten:
$C_3 = C_2 / 2$, mit $C_1 = 6.5025$ und $C_2 = 58.5225$.
Der $SSIM$ wird lokal berechnet, in Fenstern der Größe $11 \times 11$,
dabei erden die Signalwerte $x_i$ und $y_i$ mit einer Gaussianfunktion
gewichtet.
Damit lassen sich die mittlere Intensität, die Standardabweichung
und die Kovarianz zu folgenden Gleichungen umschreiben, wobei $w_i$
die Gewichtung an dem Punkt $i$ bezeichnet:

\begin{align}
	\mu_x &= \sum\limits_{i=1}^{N} w_i x_i \\
	\sigma_x &= \left( \sum\limits_{i=1}^{N} w_i(x_i - \mu_x)^2 \right)^\frac{1}{2} \\
	\sigma_{xy} &= \sum\limits_{i=1}^{N} w_i(x_i - \mu_x)(y_i - \mu_y).
\end{align}

Die Gesamtgleichung für den $SSIM(x, y)$ lässt sich durch die Gleichung

\begin{equation}
	SSIM(x, y) = \frac{(2 \mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu^2_x + \mu^2_y + C_1)(\sigma^2_x + \sigma^2_y + C_2)}
\end{equation}

ausrechnen. Der $SSIM(x, y)$ misst die Güte jedoch nur lokal, um
eine Aussage über das gesamte Bild treffen zu können, kann der
Mittelwert über allen $x \in X$ und $y \in Y$ berechnet werden:

\begin{equation}
	MSSIM(X, Y) = \frac{1}{M} \sum\limits_{j=1}^{M} SSIM(x_j, y_j).
\end{equation}

\chapter{Methodik}

In dieser Arbeit werden zwei grundsätzliche Verfahren untersucht, um die Tiefeninformationen zu übertragen
und als Dreiecksnetz darzustellen.
Der erste Ansatz besteht darin, das Tiefenbild als solches zu komprimieren und zu übertragen.
Der Klient empfängt und dekodiert das Tiefenbild und erzeugt daraus ein voll vernetztes Dreiecksnetz.
Im zweiten Ansatz wird aus dem Tiefenbild ein adaptives Dreiecksnetz mit Hilfe
der Delaunay-Triangulierung vom Server konstruiert und an den Klienten übertragen.

Die Erzeugung von adaptiven Dreiecksnetzen lässt sich als eine Form der geometrischen Kompression
bezeichnen.
Um die Anzahl der zu übertragenden Informationen weiter zu reduzieren, kann das erzeugte
Dreiecksnetz in Dreiecksstreifen zerlegt werden und sehr kompakt dargestellt werden.
Durch Valenz-basierte Kodierung kann die Informationsmenge noch weiter reduziert werden.

\textcolor{red}{TODO: nicht ganz klar was bessere kompressionsraten erreicht, werden beide Varianten untersucht.
Klares Problem, der Rechenaufwand der Vollvernetzung steigt der Auflösung deutlich an}

\section{Vollvernetzung}

Bei der Vollvernetzung wird für jeden Pixel $d$ aus dem Tiefenbild $D$ ein Vertex $v$ erzeugt.
Die entstandenen Vertices werden über Kanten zu Dreiecken miteinander verbunden.
Die Abbildung \ref{fig:FULL_MESH} zeigt drei unterschiedliche Varianten, wie sich die
Vertices zu voll vernetzten Dreiecksnetzen verbinden lassen.
Alle drei Varianten wurden implementiert, weisen bei entsprechend großen Auflösungen
jedoch keine Unterschiede hinsichtlich der Qualität der Darstellung auf.

\begin{figure}[h]
	\subfigure[]{
	\begin{tikzpicture}[scale=0.5, every node/.style={scale=0.5}]
		\begin{scope}[
			auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
			\node[vertice] (V0) at (0,0) {$0$};
			\node[vertice] (V1) at (2,0) {$1$};
			\node[vertice] (V2) at (4,0) {$2$};
			\node[vertice] (V3) at (6,0) {$3$};
			\node[vertice] (V4) at (8,0) {$4$};
			
			\node[vertice] (V5) at (0,-2) {$5$};
			\node[vertice] (V6) at (2,-2) {$6$};
			\node[vertice] (V7) at (4,-2) {$7$};
			\node[vertice] (V8) at (6,-2) {$8$};
			\node[vertice] (V9) at (8,-2) {$9$};
			
			\node[vertice] (V10) at (0,-4) {$10$};
			\node[vertice] (V11) at (2,-4) {$11$};
			\node[vertice] (V12) at (4,-4) {$12$};
			\node[vertice] (V13) at (6,-4) {$13$};
			\node[vertice] (V14) at (8,-4) {$14$};

			\node[vertice] (V15) at (0,-6) {$15$};
			\node[vertice] (V16) at (2,-6) {$16$};
			\node[vertice] (V17) at (4,-6) {$17$};
			\node[vertice] (V18) at (6,-6) {$18$};
			\node[vertice] (V19) at (8,-6) {$19$};
			
			\node[vertice] (V20) at (0,-8) {$20$};
			\node[vertice] (V21) at (2,-8) {$21$};
			\node[vertice] (V22) at (4,-8) {$22$};
			\node[vertice] (V23) at (6,-8) {$23$};
			\node[vertice] (V24) at (8,-8) {$24$};		
		\end{scope}
		
		\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V5); \draw[-] (V5) edge (V0);
		\draw[-] (V1) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V1);
		\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V6); \draw[-] (V6) edge (V1);
		\draw[-] (V2) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V2);
		\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V7); \draw[-] (V7) edge (V2);
		\draw[-] (V3) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V3);
		\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V8); \draw[-] (V8) edge (V3);
		\draw[-] (V4) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V4);
		
		\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
		\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
		\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V11);  \draw[-] (V11) edge (V6);
		\draw[-] (V7) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V7);
		\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
		\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
		\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V13);  \draw[-] (V13) edge (V8);
		\draw[-] (V9) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V9);

		\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V15); \draw[-] (V15) edge (V10);
		\draw[-] (V11) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V11);
		\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V16); \draw[-] (V16) edge (V11);
		\draw[-] (V12) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V12);
		\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V17); \draw[-] (V17) edge (V12);
		\draw[-] (V13) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V13);
		\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V18); \draw[-] (V18) edge (V13);
		\draw[-] (V14) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V14);
		
		\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
		\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V21); \draw[-] (V21) edge (V16);
		\draw[-] (V17) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V17);
		\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
		\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
		\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V23); \draw[-] (V23) edge (V18);
		\draw[-] (V19) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V19);	
	\end{tikzpicture}}
	\subfigure[]{
	\begin{tikzpicture}[scale=0.5, every node/.style={scale=0.5}]
			\begin{scope}[
				auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
				\node[vertice] (V0) at (0,0) {$0$};
				\node[vertice] (V1) at (2,0) {$1$};
				\node[vertice] (V2) at (4,0) {$2$};
				\node[vertice] (V3) at (6,0) {$3$};
				\node[vertice] (V4) at (8,0) {$4$};
				
				\node[vertice] (V5) at (0,-2) {$5$};
				\node[vertice] (V6) at (2,-2) {$6$};
				\node[vertice] (V7) at (4,-2) {$7$};
				\node[vertice] (V8) at (6,-2) {$8$};
				\node[vertice] (V9) at (8,-2) {$9$};
				
				\node[vertice] (V10) at (0,-4) {$10$};
				\node[vertice] (V11) at (2,-4) {$11$};
				\node[vertice] (V12) at (4,-4) {$12$};
				\node[vertice] (V13) at (6,-4) {$13$};
				\node[vertice] (V14) at (8,-4) {$14$};
	
				\node[vertice] (V15) at (0,-6) {$15$};
				\node[vertice] (V16) at (2,-6) {$16$};
				\node[vertice] (V17) at (4,-6) {$17$};
				\node[vertice] (V18) at (6,-6) {$18$};
				\node[vertice] (V19) at (8,-6) {$19$};
				
				\node[vertice] (V20) at (0,-8) {$20$};
				\node[vertice] (V21) at (2,-8) {$21$};
				\node[vertice] (V22) at (4,-8) {$22$};
				\node[vertice] (V23) at (6,-8) {$23$};
				\node[vertice] (V24) at (8,-8) {$24$};		
			\end{scope}
			
			\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V6); \draw[-] (V6) edge (V0);
			\draw[-] (V0) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V0);
			\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V6); \draw[-] (V6) edge (V1);
			\draw[-] (V2) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V2);
			\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V8); \draw[-] (V8) edge (V2);
			\draw[-] (V2) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V2);
			\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V8); \draw[-] (V8) edge (V3);
			\draw[-] (V4) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V4);
			
			\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
			\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
			\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V12);  \draw[-] (V12) edge (V6);
			\draw[-] (V6) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V6);
			\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
			\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
			\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V14);  \draw[-] (V14) edge (V8);
			\draw[-] (V8) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V8);
	
			\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V16); \draw[-] (V16) edge (V10);
			\draw[-] (V10) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V10);
			\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V16); \draw[-] (V16) edge (V11);
			\draw[-] (V12) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V12);
			\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V18); \draw[-] (V18) edge (V12);
			\draw[-] (V12) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V12);
			\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V18); \draw[-] (V18) edge (V13);
			\draw[-] (V14) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V14);
			
			\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
			\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
			\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V22); \draw[-] (V22) edge (V16);
			\draw[-] (V16) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V16);
			\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
			\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
			\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V24); \draw[-] (V24) edge (V18);
			\draw[-] (V18) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V18);	
		\end{tikzpicture}}
		\subfigure[]{
		\begin{tikzpicture}[scale=0.5, every node/.style={scale=0.5}]
			\begin{scope}[
				auto, vertice/.style={align=center, draw, circle, minimum width=2em, inner sep=2pt}]
				\node[vertice] (V0) at (0,0) {$0$};
				\node[vertice] (V1) at (2,0) {$1$};
				\node[vertice] (V2) at (4,0) {$2$};
				\node[vertice] (V3) at (6,0) {$3$};
				\node[vertice] (V4) at (8,0) {$4$};
				
				\node[vertice] (V5) at (0,-2) {$5$};
				\node[vertice] (V6) at (2,-2) {$6$};
				\node[vertice] (V7) at (4,-2) {$7$};
				\node[vertice] (V8) at (6,-2) {$8$};
				\node[vertice] (V9) at (8,-2) {$9$};
				
				\node[vertice] (V10) at (0,-4) {$10$};
				\node[vertice] (V11) at (2,-4) {$11$};
				\node[vertice] (V12) at (4,-4) {$12$};
				\node[vertice] (V13) at (6,-4) {$13$};
				\node[vertice] (V14) at (8,-4) {$14$};
	
				\node[vertice] (V15) at (0,-6) {$15$};
				\node[vertice] (V16) at (2,-6) {$16$};
				\node[vertice] (V17) at (4,-6) {$17$};
				\node[vertice] (V18) at (6,-6) {$18$};
				\node[vertice] (V19) at (8,-6) {$19$};
				
				\node[vertice] (V20) at (0,-8) {$20$};
				\node[vertice] (V21) at (2,-8) {$21$};
				\node[vertice] (V22) at (4,-8) {$22$};
				\node[vertice] (V23) at (6,-8) {$23$};
				\node[vertice] (V24) at (8,-8) {$24$};		
			\end{scope}
			
			\draw[-] (V0) edge (V1); \draw[-] (V1) edge (V6); \draw[-] (V6) edge (V0);
			\draw[-] (V0) edge (V6); \draw[-] (V6) edge (V5); \draw[-] (V5) edge (V0);
			\draw[-] (V1) edge (V2); \draw[-] (V2) edge (V7); \draw[-] (V7) edge (V1);
			\draw[-] (V1) edge (V7); \draw[-] (V7) edge (V6); \draw[-] (V6) edge (V1);
			\draw[-] (V2) edge (V3); \draw[-] (V3) edge (V8); \draw[-] (V8) edge (V2);
			\draw[-] (V2) edge (V8); \draw[-] (V8) edge (V7); \draw[-] (V7) edge (V2);
			\draw[-] (V3) edge (V4); \draw[-] (V4) edge (V9); \draw[-] (V9) edge (V3);
			\draw[-] (V3) edge (V9); \draw[-] (V9) edge (V8); \draw[-] (V8) edge (V3);
			
			\draw[-] (V5) edge (V6);  \draw[-] (V6) edge (V10);  \draw[-] (V10) edge (V5);
			\draw[-] (V6) edge (V11); \draw[-] (V11) edge (V10); \draw[-] (V10) edge (V6);
			\draw[-] (V6) edge (V7);  \draw[-] (V7) edge (V11);  \draw[-] (V11) edge (V6);
			\draw[-] (V7) edge (V12); \draw[-] (V12) edge (V11); \draw[-] (V11) edge (V7);
			\draw[-] (V7) edge (V8);  \draw[-] (V8) edge (V12);  \draw[-] (V12) edge (V7);
			\draw[-] (V8) edge (V13); \draw[-] (V13) edge (V12); \draw[-] (V12) edge (V8);
			\draw[-] (V8) edge (V9);  \draw[-] (V9) edge (V13);  \draw[-] (V13) edge (V8);
			\draw[-] (V9) edge (V14); \draw[-] (V14) edge (V13); \draw[-] (V13) edge (V9);
	
			\draw[-] (V10) edge (V11); \draw[-] (V11) edge (V16); \draw[-] (V16) edge (V10);
			\draw[-] (V10) edge (V16); \draw[-] (V16) edge (V15); \draw[-] (V15) edge (V10);
			\draw[-] (V11) edge (V12); \draw[-] (V12) edge (V17); \draw[-] (V17) edge (V11);
			\draw[-] (V11) edge (V17); \draw[-] (V17) edge (V16); \draw[-] (V16) edge (V11);
			\draw[-] (V12) edge (V13); \draw[-] (V13) edge (V18); \draw[-] (V18) edge (V12);
			\draw[-] (V12) edge (V18); \draw[-] (V18) edge (V17); \draw[-] (V17) edge (V12);
			\draw[-] (V13) edge (V14); \draw[-] (V14) edge (V19); \draw[-] (V19) edge (V13);
			\draw[-] (V13) edge (V19); \draw[-] (V19) edge (V18); \draw[-] (V18) edge (V13);
			
			\draw[-] (V15) edge (V16); \draw[-] (V16) edge (V20); \draw[-] (V20) edge (V15);
			\draw[-] (V16) edge (V21); \draw[-] (V21) edge (V20); \draw[-] (V20) edge (V15);
			\draw[-] (V16) edge (V17); \draw[-] (V17) edge (V21); \draw[-] (V21) edge (V16);
			\draw[-] (V17) edge (V22); \draw[-] (V22) edge (V21); \draw[-] (V21) edge (V17);
			\draw[-] (V17) edge (V18); \draw[-] (V18) edge (V22); \draw[-] (V22) edge (V17);
			\draw[-] (V18) edge (V23); \draw[-] (V23) edge (V22); \draw[-] (V22) edge (V18);
			\draw[-] (V18) edge (V19); \draw[-] (V19) edge (V23); \draw[-] (V23) edge (V18);
			\draw[-] (V19) edge (V24); \draw[-] (V24) edge (V23); \draw[-] (V23) edge (V19);	
		\end{tikzpicture}}
	\centering
	\caption{Darstellung von drei verschiedene Varianten der Vollvernetzung.}
	\label{fig:FULL_MESH}
\end{figure}

Die Anzahl der Vertices entspricht der Anzahl der Bildpunkte.
Die x- und y-Komponenten lassen sich aus der bekannten Auflösung vorausberechnen und
müssen im Falle einer Auflösungsänderung neu bestimmt werden.
Für jeden Vertex wird die z-Komponente mit Hilfe der Pixelwerte aus dem Tiefenbild
während der Bildsynthese gesetzt.
Die Anzahl der Dreiecke lässt sich dabei wie folgt berechnen: $2 (w-1) (h-1)$.

Problematisch bei diesem Verfahren ist, dass die Anzahl der Dreiecke von der Auflösung
abhängt und der Rechenaufwand der klientseitigen Bildsynthese linear mit der Auflösung steigt.

\section{Delaunay-Triangulierung}

Die zweite in dieser Arbeit untersuchte Variante, besteht in der serverseitigen Erzeugung eines adaptives Dreiecksnetz aus einem Tiefenbild $D$.
Prinzipiell, lässt sich die Vorgehensweise in drei Schritte unterteilen.
Im ersten Schritt muss eine Menge von Punkten $P \subset D$ definiert werden, sodass die Menge $P$
die Werteverläufe des Tiefenbildes möglichst optimal wiedergegeben wird.
Der zweite Schritt besteht in der Vernetzung der Punkte $p \in P$ zu Dreiecken, das dabei Eingesetze verfahren
ist die Delaunay-Triangulierung.
Zum Schluss kann das erzeugte Netz, in einem dritten Schritt, weiter verfeinert werden.

Entscheidend für das Ergebnisnetz und für die Geschwindigkeit des gesamten Verfahrens ist die Wahl der Punktmenge $P$.
Das Ziel bei der Verteilung der Punktemenge $P$ ist es, eine hohe Punktdichte im Bereich von Kanten und gekrümmten Flächen
zu erzielen.
Im Gegensatz dazu sollten Planare Regionen möglichst keine Punkte enthalten, weil diese mit wenigen Dreiecken approximiert
werden können.
Ausgangspunkt für die Erzeugung der Punktmenge $P$, sind die Gradienten des Tiefenbildes $\nabla_x$ und $\nabla_y$. 
Die Gradienten werden mit Hilfe des Sobel-Operator berechnet. 
Dazu wird die erste Ableitung berechnet und gleichzeitig wird die dazu orthogonale Richtung geglättet.
Mit Hilfe einer Faltungsmatrix der Größe $3 \times 3$ lassen sich die Gradienten berechnen:

\begin{align}
\nabla_x &= \begin{bmatrix} 1 & 0 & -1 \\ 2 & 0 & -2 \\ 1 & 0 & -1 \end{bmatrix} \ast D \\
\nabla_y &= \begin{bmatrix} 1 & 2 & 1 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{bmatrix} \ast D.
\label{eq:SOBEL_GRAD}
\end{align}

Auf diese weise entstehen für ein Tiefenbild zwei Gradientenbilder, wie in Abbildung \ref{fig:gradient_images}
dargestellt.
Zur Erzeugung der Punktmenge $P$ wurden zwei Verfahren untersucht, die im Folgenden näher betrachtet werden. 

\begin{figure}[H]
	\subfigure[]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/D10/L0.0/I0.0/gx.png}}
	\subfigure[]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/D10/L0.0/I0.0/gy.png}}
	%\subfigure[]{\includegraphics[width=.49\textwidth]{../results/6/512x512_Delaunay/D10/L0.0/I0.0/gx.png}}
	%\subfigure[]{\includegraphics[width=.49\textwidth]{../results/6/512x512_Delaunay/D10/L0.0/I0.0/gy.png}}
	\caption{Die Abbildung zeigt die Gradientenbilder beider Datensätze vom jeweils ersten Frame.
			 Dabei zeigen a und c $\nabla_x$, während b und d $\nabla_y$ der beiden Datensätze zeigt.}
	\label{fig:gradient_images}
\end{figure}

\subsection{Quadtree}

Die erste in dieser Arbeit vorgestellte Methode, zur Erzeugung der Punktmenge $P$,
basiert auf einer Quadtree-Datenstruktur.
Ein Quadtree ist eine Baumdatenstruktur, in der die Anzahl der Kindknoten, eines Knotens auf vier Kindknoten
beschränkt wird.
Im Rahmen dieser Arbeit repräsentiert jeder Knoten eine rechteckige Region $R$ des Tiefenbildes $D$.
Eine Region $R$ wird durch seine Eckpunkte parametrisiert.
Um die Baumdatenstruktur zu erzeugen, wird ein Wurzelknoten definiert, dessen Region $R = (0, 0, w-1, h-1)$ den
Ausmaßen des Tiefenbildes $D$ entspricht.
Im Anschluss wird die Region des Wurzelknotens in vier gleichgroße Teilregionen aufgeteilt.
Diese Teilregionen sind die Kindknoten des Wurzelknotens.
Alle Kindknoten werden auf diese weise sukzessiv weiter unterteilt, bis die Tiefe des erzeugten Baumes
eine maximale Baumtiefe $d_{max}$ erreicht.
Die Kindknoten, der Baumtiefe $d_{max}$ enthalten selbst keine weiteren Kindknoten und
werden als Blattknoten bezeichnet, die Regionen die sie repräsentieren als Blattregionen.
Die maximale Tiefe des Quadtrees ist dabei invers proportional zur Breite der Blattregionen.
Mit dem Parameter $d_{max}$, kann die Qualität und die Kompression des erzeugten Netzes,
sowie der Berechnungsaufwand skaliert werden.
Die erzeugte Baumdatenstruktur hängt von der Auflösung der Tiefenbilder ab
und muss nur neu erzeugt werden, wenn sich die Auflösung oder der Parameter $d_{max}$ ändert.

Im nächsten Schritt wird mit Hilfe der Baumdatenstruktur die Punktmenge $P$ erzeugt.
Zu diesem Zweck wird der Baum, beginnend mit den Blattknoten, zum Wurzelknoten traversiert.
Mit Hilfe der Gradienten kann die Region $R$ eines Knoten auf planarität getestet werden.
Ist eine Region $R$ nicht planar, dann werden die Eckpunkte von $R$ zur Menge $P$ hinzugefügt.

Um zu überprüfen, ob eine Region $R$ als planar bezeichnet werden kann, lässt sich
mit Hilfe der Differenz, zwischen dem maximalen und dem minimalen Wert der Gradienten innerhalb von $R$ bestimmten:

\begin{align}
c_x &= \max\limits_{R} \nabla_x - \min\limits_{R} \nabla_x \\
c_y &= \max\limits_{R} \nabla_y - \min\limits_{R} \nabla_y .
\end{align}

Wenn entweder $c_x$ oder $c_y$ größer als ein Schwellwert ist, dann
ist die Region $R$ nicht planar und die Eckpunkte werden der Menge $P$ hinzugefügt.
Die Werte für $c_x$ und $c_y$ der inneren Konten lassen sich effizient aus den bereits berechneten
maximalen und minimalen Gradienten seiner Kindknoten berechnen.

Die Wahl des Schwellwerts ist entscheidend für die Anzahl der eingefügten Punkte in $P$.
Dieser kann adaptiv gewählt werden, um die Komplexität des Dreiecksnetzes anzupassen an die zur Verfügung stehende Bandbreite
anzupassen.

Für den eigentlichen Planaritäts-Test unterscheidet sich dieser Schwellwert,
in Abhängigkeit, ob es sich um einen Inneren oder einen Blattknoten handelt.
Der Schwellwert für die Blattknoten wird mit $l$ bezeichnet und der für die inneren Knoten mit $i$.
Blattknoten sollten vorrangig Tiefenuntersprünge, Kanten detektieren und somit die Kontur der dargestellten Objekte.
Innere Knoten dagegen, repräsentieren die Oberfläche der Objekte und werden gesetzt, um deren Verläufe von nicht planaren
Regionen abzubilden zu können. 
Weil Tiefensprünge größere Gradienten bedeuten als gekrümmte Oberflächen, 
sollte der Schwellwert $i$, für die inneren Knoten kleiner sein, als der Schwellwert $l$ für die Blattknoten, $i < l$.
Die Werte von $i$ und $l$ liegen im Intervall $[0,1]$.

Das Ergebnis der Traversierung des Baums ist eine Menge von Punkten $P$, die sich aus den Eckpunkten der getesteten
Regionen zusammensetzt, in denen die Gradientdifferenzen den entsprechenden Schwellwert überschreitet.

Diese Punktmenge $P$ kann weiter reduziert werden, indem getestet wird, ob ein Punkt $p \in P$ zum Hintergrund gehört.
Das ist der Fall, wenn der Tiefenwert des Tiefenbildes an stelle $p$ dem maximalen Tiefenwert entspricht.
Zur Erinnerung die Tiefenwerte liegen linear zwischen den beiden Clippingebenen, wobei ein großer Wert bedeutet,
dass dieser nahe der Bildschirmebene liegt.

\subsection{Floyd-Steinberg}

Der Zweiter in dieser Arbeit vorgestellte Ansatz zur Erzeugung der Punktmenge $P$, basiert auf einem Fehler-Diffusionsverfahren.
In diesem Abschnitt wird die Bildfunktion $f(x)$ über der zweidimensionalen Bilddomain definiert.
Das Bedeutet, das die Tiefenwerte des Bildes $D$ mit der Funktion $f$ angegeben werden.
Im ersten Schritt wird aus den Gradientenbildern ein Merkmalsbild $\sigma$ erzeugt:

\begin{equation}
	\sigma(x) = \left( \frac{\parallel \nabla f(x) \parallel}{A} \right)^\gamma.
	\label{eq:featureMap}
\end{equation}

Dabei entspricht der Nenner von \ref{eq:featureMap} der Länge des Gradientenvektors, an der Stelle $x$.
Dieser wird mit $A$, dem größtmöglichen Wert von $\parallel \nabla f(x) \parallel$ normiert.
Der Parameter $\gamma \in [0,1]$ wir genutzt, um die Ausprägung von schwachen Kanten 
im Merkmalsbild $\sigma$ zu erhöhen.

Die beide Gradientenbilder $\nabla_x$ und $\nabla_y$ werden durch die Gleichung \ref{eq:featureMap} auf ein einzelnes Merkmalsbild $\sigma$ abgebildet.
Um aus dem Merkmalsbild $\sigma$ die Punktmenge $P$ zu erzeugen, kommt der Floyd-Steinberg Algorithmus zum Einsatz. 
Bei dem Floyd-Steinberg Algorithmus handelt es sich, um ein einfaches und effizientes Fehler-Diffusion-Verfahren.
Zu diesem Zweck wird das Merkmalsbild $\sigma$ Pixel für Pixel Zeilenweise durchlaufen.
Der betrachtete Pixel wird mit einem Schwellwert $\delta$ verglichen.
Wenn der Wert des Pixels größer als der Schwellwert ist, 
dann wird der Wert des Pixels mit $\delta$, andernfalls mit $0$ ersetzt.
Zusätzlich wird der neu gesetzte Wert und der daraus resultierende Quantisierungsfehler 
auf die benachbarten Pixel verteilt. 
Das Folgende Codebeispiel \ref{lst:floyd_steinberg} verdeutlicht das Verfahren:

\begin{lstlisting}[numbers=left,mathescape=true,xleftmargin=0.1\textwidth,captionpos=b,caption={Floyd-Steinberg Algorithmus},label={lst:floyd_steinberg}]
for each y
	for each x
	   oldpixel        := pixel[x][y]
	   newpixel        := (pixel[x][y] > $\delta$) ? $\delta$ : 0
	   pixel[x][y]     := newpixel
	   quant_error     := oldpixel - newpixel
	   pixel[x+1][y  ] := pixel[x+1][y  ] + quant_error * 7 / 16
	   pixel[x-1][y+1] := pixel[x-1][y+1] + quant_error * 3 / 16
	   pixel[x  ][y+1] := pixel[x  ][y+1] + quant_error * 5 / 16
	   pixel[x+1][y+1] := pixel[x+1][y+1] + quant_error * 1 / 16
\end{lstlisting}

Immer wenn die Bindung erfüllt wird und der Wert eines Pixels auf $\delta$ gesetzt wird,
dann wird die Menge $P$ mit dem gerade bearbeiteten Pixel erweitert.
Die Abbildung \ref{fig:feature_Maps} zeigt verschiedene bereits auf $\delta$ und $0$ reduzierte
Merkmalsbilder $\sigma$.

Ein Vorteil dieses Verfahrens, ist die effizienten Erzeugung der Menge $P$.
Mit Hilfe der beiden Parameter $\gamma, \delta$ lässt sich die Anzahl der Punktmenge $P$ variieren.
Dabei spielt insbesondere $\delta$ eine entscheidende Rolle, da durch diesen die Punktdichte
angepasst werden kann.
Wenn $\delta$ kleine Werte annimmt, dann wird die Punktdichte erhöht und im Fall großen Werten reduziert.

\begin{figure}[H]
	\subfigure[$\gamma = 0.2$]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/T0.5/G0.2/featuremap.png}}
	\subfigure[$\gamma = 0.4$]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/T0.5/G0.4/featuremap.png}}
	\subfigure[$\gamma = 0.6$]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/T0.5/G0.6/featuremap.png}}
	\subfigure[$\gamma = 1.0$]{\includegraphics[width=.49\textwidth]{../results/7/512x512_Delaunay/T0.5/G1.0/featuremap.png}}
	\caption{Die Bilder a bis d zeigen das Merkmalsbild, in Abhängigkeit des Parameters $\gamma$ bei einem 			 
			 festgesetzten Schwellwert von $0.5$.}
	\label{fig:feature_Maps}
\end{figure}

\subsection{Erzeugung des Netzes}

Die aus den beiden erzeugten Methoden Punkte $p \in P$ können jetzt mit Hilfe der Delaunay-Triangulierung
zu einem Dreiecksnetz miteinander verbunden werden.
In dieser Arbeit wird zu diesem Zweck die Delaunay-Triangulierung aus dem opencv-Framework verwendet.

\subsection{Netzoptimierung}

Das auf diese Weise entstandene Netze können, in Abhängigkeit der Eingabedaten, zwei arten von Artefakten enthalten.
Zum einen können Dreiecke falsche Tiefenregionen approximieren, weil nur
die Eckpunkte der jeweiligen Regionen $R$ zur Konstruktion der Dreiecke genutzt werden.
Zum anderen können Dreiecke über Tiefensprüngen liegen, wodurch Kanten nicht korrekt vom Netz abgebildet werden.
Diese Art von Artefakte kommen zustande, wenn die Blattregionen zu große Bildregionen repräsentieren.
Um die Artefakte zu reduzieren, werden nicht valide Dreiecke nochmals unterteilt oder verworfen.

Die Validität eines Dreiecks wird anhand seiner Kanten bestimmt.
Eine Kante wird als nicht valide Kante bezeichnet, wenn die Tiefenwerte
einiger von ihr überspannten Pixel abweicht, oder die 3D Richtung der Kante
sich dem Lot der Bildebene nährt, während gleichzeitig eine bestimmte Länge in der xy-Ebene überschritten wird.

Die Eckpunkte der zu prüfenden Kante, werden im folgenden mit $p_1$ und $p_2$ bezeichnet.
Der Punkt $p_m$ bezeichnet dabei den Mittelpunkt der Kante $p_1 p_2$.


Eine Kante ist nicht valide, wenn der eigentliche Tiefenwert $D(p_m)$ 
In der Gleichung \ref{eq:EDGE_VALID} approximiert der erste Term das Verhältnis,
zwischen dem Betrag des Tiefenunterschieds von $p_1$ und $p_2$
zu der Länge der Kante projiziert auf die $xy$-Ebene.

\begin{equation}
\frac{\mid d(p_1) - d(p_2) \mid}{\parallel p_1 - p_2 \parallel (d(p_1) + d(p_2))} < T_{angle}
\label{eq:EDGE_VALID}
\end{equation}

Ist der Wert des ersten Terms der Gleichung \ref{eq:EDGE_VALID} einer Kannte größer 
als der Schwellwert $T_{angle}$, dann steht diese Kante nahezu senkrecht auf der
Bildebene und sie liegt mit hoher Wahrscheinlich über einen Tiefensprung. 
Eine Kante auf die das zutrifft wird als nicht valide bezeichnet.
Enthält ein Dreieck mindestens zwei Kanten die valide sind, wird es direkt zum
endgültigen Dreiecksnetz hinzugefügt.
Wenn ein Dreieck dagegen mehr als zwei nicht valide Kanten enthält,
wird es weiter unterteilt.

Zwei Bildpunkte $p_1$ und $p_2$ werden als verbindbar bezeichnet,
wenn alle Bildpunkte zwischen $p_1$ und $p_2$ valide Tiefenwerte
besitzen und ihre Tiefe sich zum größten Teil linear ändert.
Um Rechenzeit zu sparen wird empfohlen nur den Median $p_m$ zwischen
$p_1$ und $p_2$ zu testen.
Eine Strecke wird als Verbindbar bezeichnet, wenn sie die folgende Gleichung
erfüllt:

\begin{equation}
\mid (d(p_2) - d(p_m)) - (d(p_m) - d(p_1)) \mid < T_{join}
\label{eq:EDGE_JOIN}
\end{equation}.

Um ein Dreieck zu unterteilen muss eine Fallunterscheidung durchgeführt werden.
Hat das Dreieck nur eine valide Kante $p_1 p_2$, dann müssen die anderen beiden
Kanten wie in der Abbildung \ref{fig:1VALID_EDGES} geteilt werden und es entstehen
aus dem ursprünglichen Dreieck drei neue Dreiecke.

\begin{figure}[th]
	\subfigure[]{
	\begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}]
		\draw[fill=gray!50] (0, -5) -- (0, 2) -- (2, 2) -- (2, 1) -- (5, 1) -- (5, 0) --(7, 0) -- (7, -1) -- (8, -1) -- (8, -2) -- (9, -2) -- (9, -5) -- cycle;
		
		\begin{scope}[
			auto, vertex/.style={align=center, draw, fill=white, circle, line width=0.5mm, minimum width=2em, inner sep=2pt}]
			\node[vertex] (V1) at (1, -2) {$p_1$};
			\node[vertex] (V2) at (7, -3) {$p_2$};
			\node[vertex] (V3) at (5, 4) {$p_3$};
		\end{scope}
		
		\draw[-, line width=0.5mm] (V1) edge (V2);
		\draw[-, line width=0.5mm] (V2) edge (V3);
		\draw[-, line width=0.5mm] (V1) edge (V3);
	\end{tikzpicture}}
	\subfigure[]{
	\begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}]
		\draw[fill=gray!50] (0, -5) -- (0, 2) -- (2, 2) -- (2, 1) -- (5, 1) -- (5, 0) --(7, 0) -- (7, -1) -- (8, -1) -- (8, -2) -- (9, -2) -- (9, -5) -- cycle;
		
		\begin{scope}[
			auto, vertex/.style={align=center, draw, fill=white, circle, line width=0.5mm, minimum width=2em, inner sep=5pt}]
			\node[vertex] (V1) at (1, -2) {$p_1$};
			\node[vertex] (V2) at (7, -3) {$p_2$};
			
			\node[vertex] (V13) at (2.7, 0.5) {$p_1'$};
			\node[vertex] (V23) at (6.3, -0.5) {$p_2'$};
			
			\node[vertex] (V3) at (5, 4) {$p_3$};
			
			\node[vertex] (V31) at (3.4, 1.5) {$p_3'$};
			\node[vertex] (V32) at (6, 0.5) {$p_3''$};
		\end{scope}
		
		\draw[-, line width=0.5mm] (V1) edge (V2);
		\draw[-, line width=0.5mm] (V1) edge (V13);
		\draw[-, line width=0.5mm] (V2) edge (V23);
		
		\draw[-, line width=0.5mm] (V13) edge (V23);
		\draw[-, line width=0.5mm] (V13) edge (V2);
		
		\draw[-, line width=0.5mm] (V3) edge (V31);
		\draw[-, line width=0.5mm] (V3) edge (V32);
		\draw[-, line width=0.5mm] (V31) edge (V32);
	\end{tikzpicture}}
	\centering
	\caption{Die Bilder a und b zeigen die Unterteilung des Dreiecks $p_1 p_2 p_3$, 
	wenn die zwei Kanten $p_1 p_3$, $p_2 p_3$ nicht valide sind.}
	\label{fig:1VALID_EDGES}
\end{figure}

Dazu werden vier neue Vertices iterativ entlang der alten Strecken eingefügt:

Der Vertex $p_1'$ ist der von Punkt auf der Strecke $p_3 p_1$, der am weitesten von $p_1$
entfernt liegt und mit dem Punkten $p_1$ und $p_2'$ verbindbar ist.

Der Vertex $p_2'$ ist der von Punkt auf der Strecke $p_3 p_2$, der am weitesten von $p_2$
entfernt liegt und mit dem Punkten $p_2$ und $p_1'$ verbindbar ist.

Der Vertex $p_3'$ ist der von Punkt auf der Strecke $p_3 p_1$, der am weitesten von $p_3$
entfernt liegt und mit dem Punkt $p_3$ verbindbar ist.

Der Vertex $p_3''$ ist der von Punkt auf der Strecke $p_3 p_2$, der am weitesten von $p_3$
entfernt liegt und mit dem Punkt $p_3$ verbindbar ist.

Anschließend wird das Dreieck $p_3 p_3' p_3''$ zum endgültigen Dreiecksnetz hinzugefügt.
Um die anderen beiden Dreiecke einzufügen muss eine weitere Fallunterscheidung durchgeführt
werden.
Wenn die Strecke $p_1 p_2'$ kleiner ist als $p_2 p_1'$, dann werden die Dreiecke $p_1 p_2 p_2'$,
$p_1 p_2' p_1'$ zu dem finalen Netz hinzugefügt,
andernfalls die beiden Dreiecke $p_1 p_2 p_1'$ und $p_2 p_2' p_1'$.

In dem Fall, das alle drei Kanten nicht valide sind, werden sechs neue Vertices eingefügt,
die Abbildung \ref{fig:0VALID_EDGES} verdeutlicht diesen Fall. 
Die Berechnung aller Vertices geschieht analog zu dem Vertex $p_3'$ aus dem vorhergehenden Betrachtung mit
einer validen Kante.


\begin{figure}[th]
	\subfigure[]{
	\begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}]
		\draw[fill=gray!30] (0, -5) -- (0, 2) -- (2, 2) -- (2, 1) -- (3, 1) -- (3, -5) -- cycle;
		
		\draw[fill=gray!80] (4, -5) -- (4, 1) -- (5, 1) -- (5, 0) -- (7, 0) -- (7, -1) -- (8, -1) -- (8, -2) -- (9, -2) -- (9, -5) -- cycle;
		
		\begin{scope}[
			auto, vertex/.style={align=center, draw, fill=white, circle, line width=0.5mm, minimum width=2em, inner sep=2pt}]
			\node[vertex] (V1) at (1, -2) {$p_1$};
			\node[vertex] (V2) at (7, -3) {$p_2$};
			\node[vertex] (V3) at (5, 4) {$p_3$};
		\end{scope}
	
		\draw[-, line width=0.5mm] (V1) edge (V2);
		\draw[-, line width=0.5mm] (V2) edge (V3);
		\draw[-, line width=0.5mm] (V1) edge (V3);	
	\end{tikzpicture}}
	\subfigure[]{
	\begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}]
		\draw[fill=gray!30] (0, -5) -- (0, 2) -- (2, 2) -- (2, 1) -- (3, 1) -- (3, -5) -- cycle;
		
		\draw[fill=gray!80] (4, -5) -- (4, 1) -- (5, 1) -- (5, 0) -- (7, 0) -- (7, -1) -- (8, -1) -- (8, -2) -- (9, -2) -- (9, -5) -- cycle;
		
		\begin{scope}[
			auto, vertex/.style={align=center, draw, fill=white, circle, line width=0.5mm, minimum width=2em, inner sep=2pt}]
			\node[vertex] (V1) at (1, -2) {$p_1$};
			\node[vertex] (V2) at (7, -3) {$p_2$};
			
			\node[vertex] (V13) at (2.6, 0.4) {$p_1''$};
			\node[vertex] (V12) at (2.6, -2.3) {$p_1'$};
			
			\node[vertex] (V23) at (6.3, -0.5) {$p_2'$};
			\node[vertex] (V21) at (4.5, -2.6) {$p_2''$};
			
			\node[vertex] (V3) at (5, 4) {$p_3$};
			
			\node[vertex] (V31) at (3.4, 1.5) {$p_3'$};
			\node[vertex] (V32) at (6, 0.5) {$p_3''$};
		\end{scope}
		
		\draw[-, line width=0.5mm] (V1) edge (V12);
		\draw[-, line width=0.5mm] (V1) edge (V13);
		\draw[-, line width=0.5mm] (V12) edge (V13);
		\draw[-, line width=0.5mm] (V2) edge (V23);
		\draw[-, line width=0.5mm] (V2) edge (V21);
		\draw[-, line width=0.5mm] (V21) edge (V23);
		
		\draw[-, line width=0.5mm] (V3) edge (V31);
		\draw[-, line width=0.5mm] (V3) edge (V32);
		\draw[-, line width=0.5mm] (V31) edge (V32);
	\end{tikzpicture}}
	\centering
	\caption{Die Bilder a und b zeigen die Unterteilung des Dreiecks $p_1 p_2 p_3$, 
			 wenn keine Kante valide ist.}
	\label{fig:0VALID_EDGES}
\end{figure}

Zu beachten ist, dass jedes neu erzeugte Dreieck auf Kollinearität zu überprüfen
und gegebenenfalls zu verwerfen.

Um die Schwellwerte unabhängig von der Auflösung und Farbtiefe angeben zu können,
werden die Streckenlängen in den Gleichungen \ref{eq:EDGE_JOIN}... mit Texturkoordinaten berechnet
und die Farbwerte der Pixel auf das Intervall $[0,1]$ transformiert.

\chapter{Implementierung}

Die Klient-Anwendung ist eine Browser basierte Web-Anwendung, die mit dem Server
über das WebSocket-Protokoll kommuniziert.
Dabei handelt es sich um ein auf TCP basierendes Netzwerkprotokoll,
das eine Bidirektionale Verbindung zwischen den Verbindungsteilnehmern erlaubt.
Die Extrapolation der Bilddaten wird mit Hilfe von WebGL durchgeführt wird.
Bei WebGL handelt es sich um eine Bibliothek die von modernen Browsern zur Verfügung gestellt wird,
um eine Hardware beschleunigte Bildsynthese zu ermöglichen.
Der Vorteil dieser Technologie ist die Unabhängigkeit der Anwendung im Bezug,
zur Plattform und dem Gerät, in Kombination mit einer hohen Rechengeschwindigkeit.

Beide Komponenten tauschen Informationen mit Hilfe des kompakten vom Menschen
lesbaren Java 

\subsection{16 Bit}

Farb- und Tiefenbild werden mit base64 kodiert übertragen und lassen
sich vom Browser nativ dekodieren.
Die Farbtiefe pro Kanal ist Browserseitig auf 8 Bit beschränkt.
Um 16 Bit Tiefeninformationen im Vertex-Shader nutzen zu können,
müssen diese 16 Bit auf zwei 8 Bit Kanäle aufgeteilt werden.
Die Abbildung \ref{fig:use2channels} verdeutlicht dieses Verfahren.
In den roten-Farbkanal werden die ersten 8 Bit und in den
grünen-Farbkanal die restlichen 8 Bit aufgeteilt.
Die Aufteilung der 16 Bit auf die Farbkanäle erfolgt Server-seitig.

\begin{figure}[h]
	\begin{tikzpicture}
		\node[draw=none] (A) at (0,0) {$2^{16}$};
		\node[draw=none, fill=red!20, minimum width=6cm, minimum height=1cm] (B) at (-3,-1) {$2^{8}$};
		\node[draw=none, fill=green!20, minimum width=6cm, minimum height=1cm] (C) at (3,-1) {$2^{8}$};
		\node[draw=none] (D) at (-3,-2) {heigh};
		\node[draw=none] (E) at (3, -2) {low};
		
		\draw (-6, -2.5) -- (-6, 0.5) -- (6, 0.5) -- (6, -2.5);
		\draw (-6, -0.5) -- (6, -0.5);
		\draw (-6, -1.5) -- (6, -1.5);
		\draw (-6, -2.5) -- (6, -2.5);
		\draw (0, -2.5) -- (0, -0.5);
	\end{tikzpicture}
	\centering
	\caption{Hier wird die Aufteilung einer 16 Bit Zahl auf die Farbkanäle, links rot und rechts grün, visuell verdeutlicht. Das Schlüsselwort \textit{heigh} bezeichnet die ersten 8 Bit und \textit{low} die zweiten 8 Bit.}
	\label{fig:use2channels}
\end{figure}

Um aus \textit{heigh} und \textit{low} die 16 Bit $v$ zu berechnen, genügt es den Wert von \textit{heigh}
zuerst mit 255 zu multiplizieren und dann den Wert von \textit{low} zu addieren.
Die Gleichung \ref{eq:reconstruct1} zeigt genau diesen Zusammenhang:

\begin{equation}
	v = low + heigh \times 255.
	\label{eq:reconstruct1}
\end{equation}

Beim Laden einer Textur auf die Grafikkarte, werden alle Farbkanäle normiert, im Fall
eines 8 Bit Bildes, wird jeder Wert durch den Maximalwert 255 geteilt, so dass
der Wert im Intervall von $[0, 1]$ liegt.

\begin{figure}[h]
	\begin{tikzpicture}
		\begin{scope}[auto, every node/.style={minimum width=15em}]
		 
		\node[draw, fill=lightgray!20] (A) at (0,0) {Vertex Data};
		
		\node[draw, fill=yellow]	(B) at (0,-1) {Vertex Shader};
		
		\node[draw, fill=lightgray!20] (C) at (0, -2) {Primitive Assembly};

		\node[draw, fill=lightgray!20] (D) at (0,-3) {Rasterization};
		
		\node[draw, fill=yellow] 	(E) at (0,-4) {Fragment Shader};
		
		\node[draw, fill=lightgray!20] (F) at (0,-5) {Per-Fragment Operations};
		
		\node[draw, fill=lightgray!20] (G) at (0,-6) {Framebuffer};	

		\draw[->, line width=1mm] (A) edge (B);
		\draw[->, line width=1mm] (B) edge (C);
		\draw[->, line width=1mm] (C) edge (D);
		\draw[->, line width=1mm] (D) edge (E);
		\draw[->, line width=1mm] (E) edge (F);
		\draw[->, line width=1mm] (F) edge (G);
		
		\end{scope}
	\end{tikzpicture}
	\centering
	\caption{OpengGl ES 2.0 Darstellungspipeline. 
			 Grau unterlegt sind statischen und gelb die programmierbaren Elemente.}
	\label{fig:OpenGl_pipeline}
\end{figure}

\chapter{Ergebnisse}

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in dB}, xlabel={in Grad}, xtick={0,5,10,20,...,90},
				 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
				 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
%	\addplot[green, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_7_512x512D10L0.1I0.1.csv};
%	\addplot[blue, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_6_512x512D10L0.1I0.1.csv};
	\addplot[green, only marks, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_7_512x512D10L0.1I0.1.csv};
	\addplot[blue, only marks, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_6_512x512D10L0.1I0.1.csv};
	\addplot[red, only marks, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_6_512x512Fd1651.0.csv};
	\addplot[black, only marks, mark=x] table [x=a, y=m, col sep=comma] {div_data_mean_7_512x512Fd1651.0.csv};
	\end{axis}
	\end{tikzpicture}
\end{figure}

\begin{figure}
	\begin{tikzpicture}
	\begin{axis}[axis lines = middle, enlargelimits = true, ylabel={in dB}, xlabel={in Grad}, xtick={0,5,10,20,...,90},
				 every axis y label/.style={at={(ticklabel* cs:1.05)}, anchor=east},
				 every axis x label/.style={at={(ticklabel* cs:1.05)}, anchor=south}]
%	\addplot[green, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_7_512x512D10L0.1I0.1.csv};
%	\addplot[blue, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_6_512x512D10L0.1I0.1.csv};
	\addplot[green, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_7_512x512D10L0.1I0.1.csv};
	\addplot[blue, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_6_512x512D10L0.1I0.1.csv};
	\addplot[red, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_6_512x512Fd1651.0.csv};
	\addplot[black, only marks, mark=x] table [x=a, y=p, col sep=comma] {div_data_mean_7_512x512Fd1651.0.csv};
	\end{axis}
	\end{tikzpicture}
\end{figure}

\chapter{Diskussion}

\chapter{Zusammenfassung}

\chapter{Ausblick}

\chapter{Noch mehr Ergebnisse}

\cite*{}
\end{document}